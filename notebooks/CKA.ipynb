{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interstate-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spanish-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import torch \n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sys.path.append('../tools')\n",
    "from generate import generate, sent_to_ids, load_bart, load_sents, load_dict, ids_to_tokens\n",
    "from cka import encdec_cka_sim, cal_encdec_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moved-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(sents1, sents2, num):\n",
    "    sents = [(sent1, sent2) for sent1, sent2 in zip(sents1, sents2)]\n",
    "    sampled_sents = random.sample(sents, num)\n",
    "    sampled_sents1 = [x[0] for x in sampled_sents]\n",
    "    sampled_sents2 = [x[1] for x in sampled_sents]\n",
    "    return sampled_sents1, sampled_sents2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-tanzania",
   "metadata": {},
   "source": [
    "# Japanese BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "descending-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained JaBART\n",
    "jabart_jako_path = \"../pretrained_bart/trim/jabart_jako\"\n",
    "jabart_jako_name = 'jabart_base.pt'\n",
    "pre_model = load_bart(\n",
    "    path=jabart_jako_path, model_name=jabart_jako_name\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-snapshot",
   "metadata": {},
   "source": [
    "## Korean/Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abstract-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "koja_d = load_dict(\"../pretrained_bart/trim/jabart_jako/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/jabart/koja/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/jabart/koja/dev.ko\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_ko = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-oakland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9971703786815697\n",
      "Layer 1 0.9812738013289333\n",
      "Layer 2 0.9739238246902284\n",
      "Layer 3 0.9650739860483872\n",
      "Layer 4 0.9618400017708343\n",
      "Layer 5 0.9660422979403386\n",
      "Layer 6 0.968122705004213\n",
      "Layer 7 0.9782481944354261\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.12138005263053207\n",
      "Layer 1 0.2570982162304512\n",
      "Layer 2 0.317451346899074\n",
      "Layer 3 0.34108509333253895\n",
      "Layer 4 0.38336732931309214\n",
      "Layer 5 0.3730877445713164\n",
      "Layer 6 0.14565507281983664\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.1858750434836678\n",
      "Layer 1 0.30689625563559614\n",
      "Layer 2 0.3483071697250494\n",
      "Layer 3 0.375189978250909\n",
      "Layer 4 0.3877989131905401\n",
      "Layer 5 0.3236910988351791\n",
      "elapsed_time:130.80946397781372[sec]\n"
     ]
    }
   ],
   "source": [
    "# ja-ko\n",
    "ft_path = \"../ja-ko/jabart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_ja = random_sampling(\n",
    "    sentences_ja, sentences_ja, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=koja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_ja,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominican-development",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.8828052578510951\n",
      "Layer 1 0.9040147464892014\n",
      "Layer 2 0.8906150178104792\n",
      "Layer 3 0.8838428647354586\n",
      "Layer 4 0.8944273267348168\n",
      "Layer 5 0.9063596033533926\n",
      "Layer 6 0.8884562648335584\n",
      "Layer 7 0.8915533906290435\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.15945214045884482\n",
      "Layer 1 0.2301107814456214\n",
      "Layer 2 0.2810259877645324\n",
      "Layer 3 0.30112129693878437\n",
      "Layer 4 0.3413878644049451\n",
      "Layer 5 0.3025392750966902\n",
      "Layer 6 0.12649043719278555\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.20187657079827914\n",
      "Layer 1 0.2768636300866152\n",
      "Layer 2 0.3128932577278696\n",
      "Layer 3 0.33865115253137784\n",
      "Layer 4 0.3419903773080327\n",
      "Layer 5 0.2811961032945658\n",
      "elapsed_time:125.79683542251587[sec]\n"
     ]
    }
   ],
   "source": [
    "# ko-ja\n",
    "ft_path = \"../ko-ja/jabart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_ko = random_sampling(\n",
    "    sentences_ja, sentences_ko, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=koja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_ko,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-connection",
   "metadata": {},
   "source": [
    "## English/Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "respective-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "enja_d = load_dict(\"../pretrained_bart/trim/jabart_jaen/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/jabart/enja/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_ja = f.readlines()\n",
    "    \n",
    "file_path = \"../data/jabart/enja/dev.en\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "active-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.989563281497098\n",
      "Layer 1 0.974816472648417\n",
      "Layer 2 0.9528309253401753\n",
      "Layer 3 0.9221236680843773\n",
      "Layer 4 0.905613568223484\n",
      "Layer 5 0.9197068856932752\n",
      "Layer 6 0.9353293294050234\n",
      "Layer 7 0.9439371613914803\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.1119577261480585\n",
      "Layer 1 0.19932022451944195\n",
      "Layer 2 0.29359013945335416\n",
      "Layer 3 0.35911365734848844\n",
      "Layer 4 0.43916214260604197\n",
      "Layer 5 0.3338383694447813\n",
      "Layer 6 0.1503958812159764\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.18135612039168253\n",
      "Layer 1 0.26100318634236164\n",
      "Layer 2 0.34228249247427395\n",
      "Layer 3 0.43092717671459235\n",
      "Layer 4 0.41025013328783927\n",
      "Layer 5 0.30782225709240135\n",
      "elapsed_time:110.1309769153595[sec]\n"
     ]
    }
   ],
   "source": [
    "# ja-en\n",
    "ft_path = \"../ja-en/jabart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_ja = random_sampling(\n",
    "    sentences_ja, sentences_ja, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_ja,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "injured-leather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.43952244795753864\n",
      "Layer 1 0.45148635766727835\n",
      "Layer 2 0.45407653326032255\n",
      "Layer 3 0.456910254492851\n",
      "Layer 4 0.44128720776525304\n",
      "Layer 5 0.4321100468523806\n",
      "Layer 6 0.4247472443873235\n",
      "Layer 7 0.49199936471273664\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.2187766210541609\n",
      "Layer 1 0.24496475980236715\n",
      "Layer 2 0.33469679147778675\n",
      "Layer 3 0.368891923542102\n",
      "Layer 4 0.4009444935595677\n",
      "Layer 5 0.31946105639655187\n",
      "Layer 6 0.059528893832202785\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.27436065973329726\n",
      "Layer 1 0.31041440068043336\n",
      "Layer 2 0.3700385357524329\n",
      "Layer 3 0.3913675411483885\n",
      "Layer 4 0.3791862244039186\n",
      "Layer 5 0.2610391434145364\n",
      "elapsed_time:105.03792023658752[sec]\n"
     ]
    }
   ],
   "source": [
    "# en-ja\n",
    "ft_path = \"../en-ja/jabart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_en = random_sampling(\n",
    "    sentences_ja, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_en,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-authentication",
   "metadata": {},
   "source": [
    "# English BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "scientific-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained EnBART\n",
    "enbart_enja_path = \"../pretrained_bart/trim/enbart_enja\"\n",
    "enbart_enja_name = \"enbart_base.pt\"\n",
    "pre_model = load_bart(\n",
    "    path=enbart_enja_path, model_name=enbart_enja_name\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-lawsuit",
   "metadata": {},
   "source": [
    "## Japanese/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proved-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "enja_d = load_dict(f\"{enbart_enja_path}/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/enbart/enja/dev.ja\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/enbart/enja/dev.en\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thick-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.44308230789884906\n",
      "Layer 1 0.5502702209143098\n",
      "Layer 2 0.5519916281037587\n",
      "Layer 3 0.5563345593379978\n",
      "Layer 4 0.5585082009441193\n",
      "Layer 5 0.5547125939640384\n",
      "Layer 6 0.5425837385997123\n",
      "Layer 7 0.5425837385997123\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.09931835963931872\n",
      "Layer 1 0.07595272749699429\n",
      "Layer 2 0.11041319036774082\n",
      "Layer 3 0.1846376892397597\n",
      "Layer 4 0.13568210247405774\n",
      "Layer 5 0.08839916309783269\n",
      "Layer 6 0.3417472827864746\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.08764377269044171\n",
      "Layer 1 0.15421076094913194\n",
      "Layer 2 0.22038680891153356\n",
      "Layer 3 0.23608631584197678\n",
      "Layer 4 0.20428137809692784\n",
      "Layer 5 0.1089702591060719\n",
      "elapsed_time:102.63952875137329[sec]\n"
     ]
    }
   ],
   "source": [
    "# ja-en\n",
    "ft_path = \"../ja-en/enbart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_en = random_sampling(\n",
    "    sentences_ja, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enja_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_ja,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "latin-collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9947663859591692\n",
      "Layer 1 0.9690050812960682\n",
      "Layer 2 0.9267659871455628\n",
      "Layer 3 0.9259282590568918\n",
      "Layer 4 0.9225745072622071\n",
      "Layer 5 0.9233199679951328\n",
      "Layer 6 0.9257420342116391\n",
      "Layer 7 0.9257420342116391\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.06200720237022637\n",
      "Layer 1 0.09602597670796026\n",
      "Layer 2 0.1162435341810525\n",
      "Layer 3 0.1489488699434853\n",
      "Layer 4 0.1765507707264187\n",
      "Layer 5 0.20782397281105974\n",
      "Layer 6 0.5513750252869679\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.07340509526139198\n",
      "Layer 1 0.10115642577108794\n",
      "Layer 2 0.12500983513936922\n",
      "Layer 3 0.1745240158137556\n",
      "Layer 4 0.22196037311379893\n",
      "Layer 5 0.260123068278414\n",
      "elapsed_time:165.20238304138184[sec]\n"
     ]
    }
   ],
   "source": [
    "# en-ja\n",
    "ft_path = \"../en-ja/enbart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_en, sampled_sents_en = random_sampling(\n",
    "    sentences_en, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enja_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_en,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-chinese",
   "metadata": {},
   "source": [
    "## French/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bound-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "enfr_d = load_dict(\"../pretrained_bart/trim/enbart_enfr/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/enbart/enfr/dev.fr\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_fr = f.readlines()\n",
    "\n",
    "file_path = \"../data/enbart/enfr/dev.en\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "weird-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.7646569451152809\n",
      "Layer 1 0.7587003117680807\n",
      "Layer 2 0.7757442263339923\n",
      "Layer 3 0.7663677465393556\n",
      "Layer 4 0.774393182564784\n",
      "Layer 5 0.7750444993347524\n",
      "Layer 6 0.7712746472730949\n",
      "Layer 7 0.7712746472730949\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.11365173723236892\n",
      "Layer 1 0.131481879697025\n",
      "Layer 2 0.2103302172190924\n",
      "Layer 3 0.2919080848020759\n",
      "Layer 4 0.35700600555268813\n",
      "Layer 5 0.35416613680745196\n",
      "Layer 6 0.3105485631315848\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.1047903767197506\n",
      "Layer 1 0.20801378834495848\n",
      "Layer 2 0.32268239893067757\n",
      "Layer 3 0.3840264404270085\n",
      "Layer 4 0.4018308813827713\n",
      "Layer 5 0.3303856550469767\n",
      "elapsed_time:111.59074974060059[sec]\n"
     ]
    }
   ],
   "source": [
    "# fr-en\n",
    "ft_path = \"../fr-en/enbart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_en, sampled_sents_fr = random_sampling(\n",
    "    sentences_en, sentences_fr, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enfr_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_fr,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "composed-affairs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9771762965273146\n",
      "Layer 1 0.9073712394139727\n",
      "Layer 2 0.8965725297233069\n",
      "Layer 3 0.8815354150381705\n",
      "Layer 4 0.8775547720352443\n",
      "Layer 5 0.872442251598707\n",
      "Layer 6 0.8410559203035358\n",
      "Layer 7 0.8410559203035358\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.10778351003020133\n",
      "Layer 1 0.12908465704311187\n",
      "Layer 2 0.19107924906277546\n",
      "Layer 3 0.2578092991278896\n",
      "Layer 4 0.29301651690077135\n",
      "Layer 5 0.28792259101580175\n",
      "Layer 6 0.26776594653292807\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.10354058602131933\n",
      "Layer 1 0.18931631487983047\n",
      "Layer 2 0.2836285855859728\n",
      "Layer 3 0.3269118008003395\n",
      "Layer 4 0.3352472237992844\n",
      "Layer 5 0.2687337239909468\n",
      "elapsed_time:121.1991674900055[sec]\n"
     ]
    }
   ],
   "source": [
    "# en-fr\n",
    "ft_path = \"../en-fr/enbart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_en, sampled_sents_en = random_sampling(\n",
    "    sentences_en, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enfr_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_en,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-cabinet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
