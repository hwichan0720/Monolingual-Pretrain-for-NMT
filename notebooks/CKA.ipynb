{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustained-suspension",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "engaging-resistance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subsequent-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "import pandas as pd\n",
    "from fairseq.data import Dictionary\n",
    "import numpy as np\n",
    "from fairseq.models.bart import BARTModel\n",
    "from collections import defaultdict\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swiss-jacket",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterator, Tuple, Any, Optional\n",
    "\n",
    "def forward_decoder(\n",
    "    ensemble_model,\n",
    "    tokens,\n",
    "    encoder_outs,\n",
    "    incremental_states,\n",
    "    temperature: float = 1.0,\n",
    "):\n",
    "    log_probs = []\n",
    "    avg_attn: Optional[Tensor] = None\n",
    "    encoder_out: Optional[EncoderOut] = None\n",
    "    for i, model in enumerate(ensemble_model.models):\n",
    "        if ensemble_model.has_encoder():\n",
    "            encoder_out = encoder_outs[i]\n",
    "        # decode each model\n",
    "        if ensemble_model.has_incremental_states():\n",
    "            decoder_out = _forward_decoder(\n",
    "                model.decoder,\n",
    "                tokens,\n",
    "                encoder_out=encoder_out,\n",
    "                incremental_state=incremental_states[i],\n",
    "            )\n",
    "#             decoder_out = model.decoder.forward(\n",
    "#                 tokens,\n",
    "#                 encoder_out=encoder_out,\n",
    "#                 incremental_state=incremental_states[i],\n",
    "#             )\n",
    "        else:\n",
    "            decoder_out = _forward_decoder(model.decoder, tokens, encoder_out=encoder_out)\n",
    "#             decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)\n",
    "\n",
    "        attn: Optional[Tensor] = None\n",
    "        decoder_len = len(decoder_out)\n",
    "        if decoder_len > 1 and decoder_out[1] is not None:\n",
    "            if isinstance(decoder_out[1], Tensor):\n",
    "                attn = decoder_out[1]\n",
    "            else:\n",
    "                attn_holder = decoder_out[1][\"attn\"]\n",
    "                if isinstance(attn_holder, Tensor):\n",
    "                    attn = attn_holder\n",
    "                elif attn_holder is not None:\n",
    "                    attn = attn_holder[0]\n",
    "            if attn is not None:\n",
    "                attn = attn[:, -1, :]\n",
    "\n",
    "        decoder_out_tuple = (\n",
    "            decoder_out[0][:, -1:, :].div_(temperature),\n",
    "            None if decoder_len <= 1 else decoder_out[1],\n",
    "        )\n",
    "\n",
    "        probs = model.get_normalized_probs(\n",
    "            decoder_out_tuple, log_probs=True, sample=None\n",
    "        )\n",
    "        probs = probs[:, -1, :]\n",
    "        if ensemble_model.models_size == 1:\n",
    "            return decoder_out, probs, attn\n",
    "\n",
    "        log_probs.append(probs)\n",
    "        if attn is not None:\n",
    "            if avg_attn is None:\n",
    "                avg_attn = attn\n",
    "            else:\n",
    "                avg_attn.add_(attn)\n",
    "    avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n",
    "        ensemble_model.models_size\n",
    "    )\n",
    "    if avg_attn is not None:\n",
    "        avg_attn.div_(ensemble_model.models_size)\n",
    "    return decoder_out, avg_probs, avg_attn\n",
    "\n",
    "def _generate(\n",
    "    generator,\n",
    "    sample,\n",
    "    prefix_tokens=None,\n",
    "    constraints=None,\n",
    "    bos_token=None,\n",
    "):\n",
    "    incremental_states = torch.jit.annotate(\n",
    "        List[Dict[str, Dict[str, Optional[Tensor]]]],\n",
    "        [\n",
    "            torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})\n",
    "            for i in range(generator.model.models_size)\n",
    "        ],\n",
    "    )\n",
    "    net_input = sample[\"net_input\"]\n",
    "\n",
    "    if 'src_tokens' in net_input:\n",
    "        src_tokens = net_input['src_tokens']\n",
    "        # length of the source text being the character length except EndOfSentence and pad\n",
    "        src_lengths = (src_tokens.ne(generator.eos) & src_tokens.ne(generator.pad)).long().sum(dim=1)\n",
    "    elif 'source' in net_input:\n",
    "        src_tokens = net_input['source']\n",
    "        src_lengths = (\n",
    "            net_input['padding_mask'].size(-1) - net_input['padding_mask'].sum(-1)\n",
    "            if net_input['padding_mask'] is not None\n",
    "            else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('expected src_tokens or source in net input')\n",
    "\n",
    "    # bsz: total number of sentences in beam\n",
    "    # Note that src_tokens may have more than 2 dimenions (i.e. audio features)\n",
    "    bsz, src_len = src_tokens.size()[:2]\n",
    "    beam_size = generator.beam_size\n",
    "\n",
    "    if constraints is not None and not generator.search.supports_constraints:\n",
    "        raise NotImplementedError(\"Target-side constraints were provided, but search method doesn't support them\")\n",
    "\n",
    "    # Initialize constraints, when active\n",
    "    generator.search.init_constraints(constraints, beam_size)\n",
    "\n",
    "    max_len: int = -1\n",
    "    if generator.match_source_len:\n",
    "        max_len = src_lengths.max().item()\n",
    "    else:\n",
    "        max_len = min(\n",
    "            int(generator.max_len_a * src_len + generator.max_len_b),\n",
    "            # exclude the EOS marker\n",
    "            generator.model.max_decoder_positions() - 1,\n",
    "        )\n",
    "    assert (\n",
    "        generator.min_len <= max_len\n",
    "    ), \"min_len cannot be larger than max_len, please adjust these!\"\n",
    "    # compute the encoder output for each beam\n",
    "#     encoder_outs = generator.model.forward_encoder(net_input, return_all_hiddens=True)\n",
    "\n",
    "#     # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\n",
    "#     new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "#     new_order = new_order.to(src_tokens.device).long()\n",
    "#     encoder_outs = generator.model.reorder_encoder_out(encoder_outs, new_order)\n",
    "\n",
    "    encoder_outs = [generator.model.models[0].encoder(src_tokens,  None, return_all_hiddens=True)]\n",
    "    \n",
    "    # ensure encoder_outs is a List.\n",
    "    assert encoder_outs is not None\n",
    "\n",
    "    # initialize buffers\n",
    "    scores = (\n",
    "        torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()\n",
    "    )  # +1 for eos; pad is never chosen for scoring\n",
    "    tokens = (\n",
    "        torch.zeros(bsz * beam_size, max_len + 2)\n",
    "        .to(src_tokens)\n",
    "        .long()\n",
    "        .fill_(generator.pad)\n",
    "    )  # +2 for eos and pad\n",
    "    tokens[:, 0] = generator.eos if bos_token is None else bos_token\n",
    "    attn: Optional[Tensor] = None\n",
    "\n",
    "    # A list that indicates candidates that should be ignored.\n",
    "    # For example, suppose we're sampling and have already finalized 2/5\n",
    "    # samples. Then cands_to_ignore would mark 2 positions as being ignored,\n",
    "    # so that we only finalize the remaining 3 samples.\n",
    "    cands_to_ignore = (\n",
    "        torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)\n",
    "    )  # forward and backward-compatible False mask\n",
    "\n",
    "    # list of completed sentences\n",
    "    finalized = torch.jit.annotate(\n",
    "        List[List[Dict[str, Tensor]]],\n",
    "        [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],\n",
    "    )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step\n",
    "\n",
    "    finished = [\n",
    "        False for i in range(bsz)\n",
    "    ]  # a boolean array indicating if the sentence at the index is finished or not\n",
    "    num_remaining_sent = bsz  # number of sentences remaining\n",
    "\n",
    "    # number of candidate hypos per step\n",
    "    cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n",
    "\n",
    "    # offset arrays for converting between different indexing schemes\n",
    "    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n",
    "    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n",
    "\n",
    "    reorder_state: Optional[Tensor] = None\n",
    "    batch_idxs: Optional[Tensor] = None\n",
    "        \n",
    "    decoder_outs = []\n",
    "    for step in range(max_len + 1):  # one extra step for EOS marker\n",
    "#         print(tokens[:, : step + 1])\n",
    "        decoder_out, lprobs, avg_attn_scores = forward_decoder(\n",
    "            generator.model,\n",
    "            tokens[:, : step + 1],\n",
    "            encoder_outs,\n",
    "            incremental_states,\n",
    "            generator.temperature,\n",
    "        )\n",
    "        \n",
    "#         print(lprobs.shape)\n",
    "#         print(lprobs)\n",
    "        decoder_outs.append(decoder_out)\n",
    "        \n",
    "        lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\n",
    "\n",
    "        lprobs[:, generator.pad] = -math.inf  # never select pad\n",
    "        lprobs[:, generator.unk] -= generator.unk_penalty  # apply unk penalty\n",
    "\n",
    "        # handle max length constraint\n",
    "        if step >= max_len:\n",
    "            lprobs[:, : generator.eos] = -math.inf\n",
    "            lprobs[:, generator.eos + 1 :] = -math.inf\n",
    "\n",
    "        # handle prefix tokens (possibly with different lengths)\n",
    "        if (\n",
    "            prefix_tokens is not None\n",
    "            and step < prefix_tokens.size(1)\n",
    "            and step < max_len\n",
    "        ):\n",
    "            lprobs, tokens, scores = generator._prefix_tokens(\n",
    "                step, lprobs, scores, tokens, prefix_tokens, beam_size\n",
    "            )\n",
    "        elif step < generator.min_len:\n",
    "            # minimum length constraint (does not apply if using prefix_tokens)\n",
    "            lprobs[:, generator.eos] = -math.inf\n",
    "\n",
    "        # Record attention scores, only support avg_attn_scores is a Tensor\n",
    "        if avg_attn_scores is not None:\n",
    "            if attn is None:\n",
    "                attn = torch.empty(\n",
    "                    bsz * beam_size, avg_attn_scores.size(1), max_len + 2\n",
    "                ).to(scores)\n",
    "            attn[:, :, step + 1].copy_(avg_attn_scores)\n",
    "\n",
    "        scores = scores.type_as(lprobs)\n",
    "        eos_bbsz_idx = torch.empty(0).to(\n",
    "            tokens\n",
    "        )  # indices of hypothesis ending with eos (finished sentences)\n",
    "        eos_scores = torch.empty(0).to(\n",
    "            scores\n",
    "        )  # scores of hypothesis ending with eos (finished sentences)\n",
    "\n",
    "        if generator.should_set_src_lengths:\n",
    "            generator.search.set_src_lengths(src_lengths)\n",
    "\n",
    "        if generator.no_repeat_ngram_size > 0:\n",
    "            lprobs = generator._no_repeat_ngram(tokens, lprobs, bsz, beam_size, step)\n",
    "\n",
    "        # Shape: (batch, cand_size)\n",
    "        cand_scores, cand_indices, cand_beams = generator.search.step(\n",
    "            step,\n",
    "            lprobs.view(bsz, -1, generator.vocab_size),\n",
    "            scores.view(bsz, beam_size, -1)[:, :, :step],\n",
    "        )\n",
    "\n",
    "        # cand_bbsz_idx contains beam indices for the top candidate\n",
    "        # hypotheses, with a range of values: [0, bsz*beam_size),\n",
    "        # and dimensions: [bsz, cand_size]\n",
    "        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n",
    "\n",
    "        # finalize hypotheses that end in eos\n",
    "        # Shape of eos_mask: (batch size, beam size)\n",
    "        eos_mask = cand_indices.eq(generator.eos) & cand_scores.ne(-math.inf)\n",
    "        eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0).to(eos_mask)\n",
    "\n",
    "        # only consider eos when it's among the top beam_size indices\n",
    "        # Now we know what beam item(s) to finish\n",
    "        # Shape: 1d list of absolute-numbered\n",
    "        eos_bbsz_idx = torch.masked_select(\n",
    "            cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]\n",
    "        )\n",
    "\n",
    "        finalized_sents: List[int] = []\n",
    "        if eos_bbsz_idx.numel() > 0:\n",
    "            eos_scores = torch.masked_select(\n",
    "                cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]\n",
    "            )\n",
    "\n",
    "            finalized_sents = generator.finalize_hypos(\n",
    "                step,\n",
    "                eos_bbsz_idx,\n",
    "                eos_scores,\n",
    "                tokens,\n",
    "                scores,\n",
    "                finalized,\n",
    "                finished,\n",
    "                beam_size,\n",
    "                attn,\n",
    "                src_lengths,\n",
    "                max_len,\n",
    "            )\n",
    "            num_remaining_sent -= len(finalized_sents)\n",
    "\n",
    "        assert num_remaining_sent >= 0\n",
    "        if num_remaining_sent == 0:\n",
    "            break\n",
    "        assert step < max_len\n",
    "\n",
    "        # Remove finalized sentences (ones for which {beam_size}\n",
    "        # finished hypotheses have been generated) from the batch.\n",
    "        if len(finalized_sents) > 0:\n",
    "            new_bsz = bsz - len(finalized_sents)\n",
    "\n",
    "            # construct batch_idxs which holds indices of batches to keep for the next pass\n",
    "            batch_mask = torch.ones(bsz, dtype=torch.bool, device=cand_indices.device)\n",
    "            batch_mask[finalized_sents] = False\n",
    "            # TODO replace `nonzero(as_tuple=False)` after TorchScript supports it\n",
    "            batch_idxs = torch.arange(bsz, device=cand_indices.device).masked_select(batch_mask)\n",
    "\n",
    "            # Choose the subset of the hypothesized constraints that will continue\n",
    "            generator.search.prune_sentences(batch_idxs)\n",
    "\n",
    "            eos_mask = eos_mask[batch_idxs]\n",
    "            cand_beams = cand_beams[batch_idxs]\n",
    "            bbsz_offsets.resize_(new_bsz, 1)\n",
    "            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n",
    "            cand_scores = cand_scores[batch_idxs]\n",
    "            cand_indices = cand_indices[batch_idxs]\n",
    "\n",
    "            if prefix_tokens is not None:\n",
    "                prefix_tokens = prefix_tokens[batch_idxs]\n",
    "            src_lengths = src_lengths[batch_idxs]\n",
    "            cands_to_ignore = cands_to_ignore[batch_idxs]\n",
    "\n",
    "            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n",
    "            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n",
    "            if attn is not None:\n",
    "                attn = attn.view(bsz, -1)[batch_idxs].view(\n",
    "                    new_bsz * beam_size, attn.size(1), -1\n",
    "                )\n",
    "            bsz = new_bsz\n",
    "        else:\n",
    "            batch_idxs = None\n",
    "\n",
    "        # Set active_mask so that values > cand_size indicate eos hypos\n",
    "        # and values < cand_size indicate candidate active hypos.\n",
    "        # After, the min values per row are the top candidate active hypos\n",
    "\n",
    "        # Rewrite the operator since the element wise or is not supported in torchscript.\n",
    "\n",
    "        eos_mask[:, :beam_size] = ~((~cands_to_ignore) & (~eos_mask[:, :beam_size]))\n",
    "        active_mask = torch.add(\n",
    "            eos_mask.type_as(cand_offsets) * cand_size,\n",
    "            cand_offsets[: eos_mask.size(1)],\n",
    "        )\n",
    "\n",
    "        # get the top beam_size active hypotheses, which are just\n",
    "        # the hypos with the smallest values in active_mask.\n",
    "        # {active_hypos} indicates which {beam_size} hypotheses\n",
    "        # from the list of {2 * beam_size} candidates were\n",
    "        # selected. Shapes: (batch size, beam size)\n",
    "        new_cands_to_ignore, active_hypos = torch.topk(\n",
    "            active_mask, k=beam_size, dim=1, largest=False\n",
    "        )\n",
    "\n",
    "        # update cands_to_ignore to ignore any finalized hypos.\n",
    "        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n",
    "        # Make sure there is at least one active item for each sentence in the batch.\n",
    "        assert (~cands_to_ignore).any(dim=1).all()\n",
    "\n",
    "        # update cands_to_ignore to ignore any finalized hypos\n",
    "\n",
    "        # {active_bbsz_idx} denotes which beam number is continued for each new hypothesis (a beam\n",
    "        # can be selected more than once).\n",
    "        active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)\n",
    "        active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)\n",
    "\n",
    "        active_bbsz_idx = active_bbsz_idx.view(-1)\n",
    "        active_scores = active_scores.view(-1)\n",
    "\n",
    "        # copy tokens and scores for active hypotheses\n",
    "\n",
    "        # Set the tokens for each beam (can select the same row more than once)\n",
    "        tokens[:, : step + 1] = torch.index_select(\n",
    "            tokens[:, : step + 1], dim=0, index=active_bbsz_idx\n",
    "        )\n",
    "        # Select the next token for each of them\n",
    "        tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(\n",
    "            cand_indices, dim=1, index=active_hypos\n",
    "        )\n",
    "        if step > 0:\n",
    "            scores[:, :step] = torch.index_select(\n",
    "                scores[:, :step], dim=0, index=active_bbsz_idx\n",
    "            )\n",
    "        scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(\n",
    "            cand_scores, dim=1, index=active_hypos\n",
    "        )\n",
    "\n",
    "        # Update constraints based on which candidates were selected for the next beam\n",
    "        generator.search.update_constraints(active_hypos)\n",
    "\n",
    "        # copy attention for active hypotheses\n",
    "        if attn is not None:\n",
    "            attn[:, :, : step + 2] = torch.index_select(\n",
    "                attn[:, :, : step + 2], dim=0, index=active_bbsz_idx\n",
    "            )\n",
    "\n",
    "        # reorder incremental state in decoder\n",
    "        reorder_state = active_bbsz_idx\n",
    "\n",
    "    # sort by score descending\n",
    "    for sent in range(len(finalized)):\n",
    "        scores = torch.tensor([float(elem[\"score\"].item()) for elem in finalized[sent]])\n",
    "        _, sorted_scores_indices = torch.sort(scores, descending=True)\n",
    "        finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]\n",
    "        finalized[sent] = torch.jit.annotate(List[Dict[str, Tensor]], finalized[sent])\n",
    "    return encoder_outs, decoder_outs, finalized\n",
    "\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "negative-chance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _forward_decoder(\n",
    "    decoder,\n",
    "    prev_output_tokens,\n",
    "    encoder_out,\n",
    "    incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "    full_context_alignment: bool = False,\n",
    "    alignment_layer: Optional[int] = None,\n",
    "    alignment_heads: Optional[int] = None,\n",
    "):\n",
    "    if alignment_layer is None:\n",
    "        alignment_layer = decoder.num_layers - 1\n",
    "\n",
    "    # embed positions\n",
    "    positions = (\n",
    "        decoder.embed_positions(\n",
    "            prev_output_tokens, incremental_state=incremental_state\n",
    "        )\n",
    "        if decoder.embed_positions is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if incremental_state is not None:\n",
    "        prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "        if positions is not None:\n",
    "            positions = positions[:, -1:]\n",
    "\n",
    "    # embed tokens and positions\n",
    "    x = decoder.embed_scale * decoder.embed_tokens(prev_output_tokens)\n",
    "\n",
    "    if decoder.quant_noise is not None:\n",
    "        x = decoder.quant_noise(x)\n",
    "\n",
    "    if decoder.project_in_dim is not None:\n",
    "        x = decoder.project_in_dim(x)\n",
    "\n",
    "    if positions is not None:\n",
    "        x += positions\n",
    "\n",
    "    if decoder.layernorm_embedding is not None:\n",
    "        x = decoder.layernorm_embedding(x)\n",
    "\n",
    "    x = decoder.dropout_module(x)\n",
    "\n",
    "    # B x T x C -> T x B x C\n",
    "    x = x.transpose(0, 1)\n",
    "\n",
    "    self_attn_padding_mask: Optional[Tensor] = None\n",
    "    if decoder.cross_self_attention or prev_output_tokens.eq(decoder.padding_idx).any():\n",
    "        self_attn_padding_mask = prev_output_tokens.eq(decoder.padding_idx)\n",
    "\n",
    "    # decoder layers\n",
    "    attn: Optional[Tensor] = None\n",
    "    inner_states: List[Optional[Tensor]] = [x]\n",
    "    self_dec_attns = []\n",
    "    for idx, layer in enumerate(decoder.layers):\n",
    "        if incremental_state is None and not full_context_alignment:\n",
    "            self_attn_mask = decoder.buffered_future_mask(x)\n",
    "        else:\n",
    "            self_attn_mask = None\n",
    "\n",
    "        x, layer_attn, self_dec_attn, _ = _forward_layer(\n",
    "            layer,\n",
    "            x,\n",
    "            encoder_out.encoder_out if encoder_out is not None else None,\n",
    "            encoder_out.encoder_padding_mask if encoder_out is not None else None,\n",
    "            incremental_state,\n",
    "            self_attn_mask=self_attn_mask,\n",
    "            self_attn_padding_mask=self_attn_padding_mask,\n",
    "            need_attn=bool((idx == alignment_layer)),\n",
    "            need_head_weights=bool((idx == alignment_layer)),\n",
    "        )\n",
    "        inner_states.append(x)\n",
    "        self_dec_attns.append(self_dec_attn)\n",
    "        if layer_attn is not None and idx == alignment_layer:\n",
    "            attn = layer_attn.float().to(x)\n",
    "\n",
    "    if attn is not None:\n",
    "        if alignment_heads is not None:\n",
    "            attn = attn[:alignment_heads]\n",
    "\n",
    "        # average probabilities over heads\n",
    "        attn = attn.mean(dim=0)\n",
    "\n",
    "    if decoder.layer_norm is not None:\n",
    "        x = decoder.layer_norm(x)\n",
    "\n",
    "    # T x B x C -> B x T x C\n",
    "    x = x.transpose(0, 1)\n",
    "\n",
    "    if decoder.project_out_dim is not None:\n",
    "        x = decoder.project_out_dim(x)\n",
    "    \n",
    "    x = decoder.output_layer(x)\n",
    "\n",
    "    return x, {\"attn\": [attn], \"inner_states\": inner_states, \"self_dec_attns\": self_dec_attns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "photographic-israel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _forward_layer(\n",
    "    layer,\n",
    "    x,\n",
    "    encoder_out: Optional[torch.Tensor] = None,\n",
    "    encoder_padding_mask: Optional[torch.Tensor] = None,\n",
    "    incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "    prev_self_attn_state: Optional[List[torch.Tensor]] = None,\n",
    "    prev_attn_state: Optional[List[torch.Tensor]] = None,\n",
    "    self_attn_mask: Optional[torch.Tensor] = None,\n",
    "    self_attn_padding_mask: Optional[torch.Tensor] = None,\n",
    "    need_attn: bool = False,\n",
    "    need_head_weights: bool = False,\n",
    "):\n",
    "    if need_head_weights:\n",
    "            need_attn = True\n",
    "\n",
    "    residual = x\n",
    "    if layer.normalize_before:\n",
    "        x = layer.self_attn_layer_norm(x)\n",
    "    if prev_self_attn_state is not None:\n",
    "        prev_key, prev_value = prev_self_attn_state[:2]\n",
    "        saved_state: Dict[str, Optional[Tensor]] = {\n",
    "            \"prev_key\": prev_key,\n",
    "            \"prev_value\": prev_value,\n",
    "        }\n",
    "        if len(prev_self_attn_state) >= 3:\n",
    "            saved_state[\"prev_key_padding_mask\"] = prev_self_attn_state[2]\n",
    "        assert incremental_state is not None\n",
    "        layer.self_attn._set_input_buffer(incremental_state, saved_state)\n",
    "    _self_attn_input_buffer = layer.self_attn._get_input_buffer(incremental_state)\n",
    "    if layer.cross_self_attention and not (\n",
    "        incremental_state is not None\n",
    "        and _self_attn_input_buffer is not None\n",
    "        and \"prev_key\" in _self_attn_input_buffer\n",
    "    ):\n",
    "        if self_attn_mask is not None:\n",
    "            assert encoder_out is not None\n",
    "            self_attn_mask = torch.cat(\n",
    "                (x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1\n",
    "            )\n",
    "        if self_attn_padding_mask is not None:\n",
    "            if encoder_padding_mask is None:\n",
    "                assert encoder_out is not None\n",
    "                encoder_padding_mask = self_attn_padding_mask.new_zeros(\n",
    "                    encoder_out.size(1), encoder_out.size(0)\n",
    "                )\n",
    "            self_attn_padding_mask = torch.cat(\n",
    "                (encoder_padding_mask, self_attn_padding_mask), dim=1\n",
    "            )\n",
    "        assert encoder_out is not None\n",
    "        y = torch.cat((encoder_out, x), dim=0)\n",
    "    else:\n",
    "        y = x\n",
    "\n",
    "    x, attn = layer.self_attn(\n",
    "        query=x,\n",
    "        key=y,\n",
    "        value=y,\n",
    "        key_padding_mask=self_attn_padding_mask,\n",
    "        incremental_state=incremental_state,\n",
    "        need_weights=False,\n",
    "        attn_mask=self_attn_mask,\n",
    "    )\n",
    "    x = layer.dropout_module(x)\n",
    "    x = residual + x\n",
    "    if not layer.normalize_before:\n",
    "        x = layer.self_attn_layer_norm(x)\n",
    "        \n",
    "    self_dec_attn = copy.copy(x)\n",
    "\n",
    "    if layer.encoder_attn is not None and encoder_out is not None:\n",
    "        residual = x\n",
    "        if layer.normalize_before:\n",
    "            x = layer.encoder_attn_layer_norm(x)\n",
    "        if prev_attn_state is not None:\n",
    "            prev_key, prev_value = prev_attn_state[:2]\n",
    "            saved_state: Dict[str, Optional[Tensor]] = {\n",
    "                \"prev_key\": prev_key,\n",
    "                \"prev_value\": prev_value,\n",
    "            }\n",
    "            if len(prev_attn_state) >= 3:\n",
    "                saved_state[\"prev_key_padding_mask\"] = prev_attn_state[2]\n",
    "            assert incremental_state is not None\n",
    "            layer.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
    "\n",
    "        x, attn = layer.encoder_attn(\n",
    "            query=x,\n",
    "            key=encoder_out,\n",
    "            value=encoder_out,\n",
    "            key_padding_mask=encoder_padding_mask,\n",
    "            incremental_state=incremental_state,\n",
    "            static_kv=True,\n",
    "            need_weights=need_attn or (not layer.training and layer.need_attn),\n",
    "            need_head_weights=need_head_weights,\n",
    "        )\n",
    "        x = layer.dropout_module(x)\n",
    "        x = residual + x\n",
    "        if not layer.normalize_before:\n",
    "            x = layer.encoder_attn_layer_norm(x)\n",
    "\n",
    "    residual = x\n",
    "    if layer.normalize_before:\n",
    "        x = layer.final_layer_norm(x)\n",
    "\n",
    "    x = layer.activation_fn(layer.fc1(x))\n",
    "    x = layer.activation_dropout_module(x)\n",
    "    x = layer.fc2(x)\n",
    "    x = layer.dropout_module(x)\n",
    "    x = residual + x\n",
    "    if not layer.normalize_before:\n",
    "        x = layer.final_layer_norm(x)\n",
    "    if layer.onnx_trace and incremental_state is not None:\n",
    "        saved_state = layer.self_attn._get_input_buffer(incremental_state)\n",
    "        assert saved_state is not None\n",
    "        if self_attn_padding_mask is not None:\n",
    "            self_attn_state = [\n",
    "                saved_state[\"prev_key\"],\n",
    "                saved_state[\"prev_value\"],\n",
    "                saved_state[\"prev_key_padding_mask\"],\n",
    "            ]\n",
    "        else:\n",
    "            self_attn_state = [saved_state[\"prev_key\"], saved_state[\"prev_value\"]]\n",
    "        return x, attn, self_attn_state, None\n",
    "    return x, attn, self_dec_attn, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "superior-spank",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def _generate_unsupervised(\n",
    "    enc_generator,\n",
    "    dec_generator,\n",
    "    sample,\n",
    "    prefix_tokens=None,\n",
    "    constraints=None,\n",
    "    bos_token=None,\n",
    "):\n",
    "    incremental_states = torch.jit.annotate(\n",
    "        List[Dict[str, Dict[str, Optional[Tensor]]]],\n",
    "        [\n",
    "            torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})\n",
    "            for i in range(dec_generator.model.models_size)\n",
    "        ],\n",
    "    )\n",
    "    net_input = sample[\"net_input\"]\n",
    "\n",
    "    if 'src_tokens' in net_input:\n",
    "        src_tokens = net_input['src_tokens']\n",
    "        # length of the source text being the character length except EndOfSentence and pad\n",
    "        src_lengths = (src_tokens.ne(dec_generator.eos) & src_tokens.ne(dec_generator.pad)).long().sum(dim=1)\n",
    "    elif 'source' in net_input:\n",
    "        src_tokens = net_input['source']\n",
    "        src_lengths = (\n",
    "            net_input['padding_mask'].size(-1) - net_input['padding_mask'].sum(-1)\n",
    "            if net_input['padding_mask'] is not None\n",
    "            else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('expected src_tokens or source in net input')\n",
    "\n",
    "    # bsz: total number of sentences in beam\n",
    "    # Note that src_tokens may have more than 2 dimenions (i.e. audio features)\n",
    "    bsz, src_len = src_tokens.size()[:2]\n",
    "    beam_size = dec_generator.beam_size\n",
    "\n",
    "    if constraints is not None and not dec_generator.search.supports_constraints:\n",
    "        raise NotImplementedError(\"Target-side constraints were provided, but search method doesn't support them\")\n",
    "\n",
    "    # Initialize constraints, when active\n",
    "    dec_generator.search.init_constraints(constraints, beam_size)\n",
    "\n",
    "    max_len: int = -1\n",
    "    if dec_generator.match_source_len:\n",
    "        max_len = src_lengths.max().item()\n",
    "    else:\n",
    "        max_len = min(\n",
    "            int(dec_generator.max_len_a * src_len + dec_generator.max_len_b),\n",
    "            # exclude the EOS marker\n",
    "            dec_generator.model.max_decoder_positions() - 1,\n",
    "        )\n",
    "    assert (\n",
    "        dec_generator.min_len <= max_len\n",
    "    ), \"min_len cannot be larger than max_len, please adjust these!\"\n",
    "    # compute the encoder output for each beam\n",
    "    encoder_outs = enc_generator.model.forward_encoder(net_input)\n",
    "\n",
    "    # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\n",
    "    new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "    new_order = new_order.to(src_tokens.device).long()\n",
    "    encoder_outs = enc_generator.model.reorder_encoder_out(encoder_outs, new_order)\n",
    "\n",
    "#     encoder_outs = [enc_model.model.encoder(src_tokens,  None, return_all_hiddens=True)]\n",
    "    \n",
    "    # ensure encoder_outs is a List.\n",
    "    assert encoder_outs is not None\n",
    "\n",
    "    # initialize buffers\n",
    "    scores = (\n",
    "        torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()\n",
    "    )  # +1 for eos; pad is never chosen for scoring\n",
    "    tokens = (\n",
    "        torch.zeros(bsz * beam_size, max_len + 2)\n",
    "        .to(src_tokens)\n",
    "        .long()\n",
    "        .fill_(dec_generator.pad)\n",
    "    )  # +2 for eos and pad\n",
    "    tokens[:, 0] = dec_generator.eos if bos_token is None else bos_token\n",
    "    attn: Optional[Tensor] = None\n",
    "\n",
    "    # A list that indicates candidates that should be ignored.\n",
    "    # For example, suppose we're sampling and have already finalized 2/5\n",
    "    # samples. Then cands_to_ignore would mark 2 positions as being ignored,\n",
    "    # so that we only finalize the remaining 3 samples.\n",
    "    cands_to_ignore = (\n",
    "        torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)\n",
    "    )  # forward and backward-compatible False mask\n",
    "\n",
    "    # list of completed sentences\n",
    "    finalized = torch.jit.annotate(\n",
    "        List[List[Dict[str, Tensor]]],\n",
    "        [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],\n",
    "    )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step\n",
    "\n",
    "    finished = [\n",
    "        False for i in range(bsz)\n",
    "    ]  # a boolean array indicating if the sentence at the index is finished or not\n",
    "    num_remaining_sent = bsz  # number of sentences remaining\n",
    "\n",
    "    # number of candidate hypos per step\n",
    "    cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n",
    "\n",
    "    # offset arrays for converting between different indexing schemes\n",
    "    bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n",
    "    cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n",
    "\n",
    "    reorder_state: Optional[Tensor] = None\n",
    "    batch_idxs: Optional[Tensor] = None\n",
    "        \n",
    "    decoder_outs = []\n",
    "    for step in range(max_len + 1):  # one extra step for EOS marker\n",
    "        \n",
    "        decoder_out, lprobs, avg_attn_scores = forward_decoder(\n",
    "            dec_generator.model,\n",
    "            tokens[:, : step + 1],\n",
    "            encoder_outs,\n",
    "            incremental_states,\n",
    "            dec_generator.temperature,\n",
    "        )\n",
    "        \n",
    "        decoder_outs.append(decoder_out)\n",
    "        \n",
    "        lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\n",
    "\n",
    "        lprobs[:, dec_generator.pad] = -math.inf  # never select pad\n",
    "        lprobs[:, dec_generator.unk] -= dec_generator.unk_penalty  # apply unk penalty\n",
    "\n",
    "        # handle max length constraint\n",
    "        if step >= max_len:\n",
    "            lprobs[:, : dec_generator.eos] = -math.inf\n",
    "            lprobs[:, dec_generator.eos + 1 :] = -math.inf\n",
    "\n",
    "        # handle prefix tokens (possibly with different lengths)\n",
    "        if (\n",
    "            prefix_tokens is not None\n",
    "            and step < prefix_tokens.size(1)\n",
    "            and step < max_len\n",
    "        ):\n",
    "            lprobs, tokens, scores = dec_generator._prefix_tokens(\n",
    "                step, lprobs, scores, tokens, prefix_tokens, beam_size\n",
    "            )\n",
    "        elif step < dec_generator.min_len:\n",
    "            # minimum length constraint (does not apply if using prefix_tokens)\n",
    "            lprobs[:, dec_generator.eos] = -math.inf\n",
    "\n",
    "        # Record attention scores, only support avg_attn_scores is a Tensor\n",
    "        if avg_attn_scores is not None:\n",
    "            if attn is None:\n",
    "                attn = torch.empty(\n",
    "                    bsz * beam_size, avg_attn_scores.size(1), max_len + 2\n",
    "                ).to(scores)\n",
    "            attn[:, :, step + 1].copy_(avg_attn_scores)\n",
    "\n",
    "        scores = scores.type_as(lprobs)\n",
    "        eos_bbsz_idx = torch.empty(0).to(\n",
    "            tokens\n",
    "        )  # indices of hypothesis ending with eos (finished sentences)\n",
    "        eos_scores = torch.empty(0).to(\n",
    "            scores\n",
    "        )  # scores of hypothesis ending with eos (finished sentences)\n",
    "\n",
    "        if dec_generator.should_set_src_lengths:\n",
    "            dec_generator.search.set_src_lengths(src_lengths)\n",
    "\n",
    "        if dec_generator.no_repeat_ngram_size > 0:\n",
    "            lprobs = dec_generator._no_repeat_ngram(tokens, lprobs, bsz, beam_size, step)\n",
    "\n",
    "        # Shape: (batch, cand_size)\n",
    "        cand_scores, cand_indices, cand_beams = dec_generator.search.step(\n",
    "            step,\n",
    "            lprobs.view(bsz, -1, dec_generator.vocab_size),\n",
    "            scores.view(bsz, beam_size, -1)[:, :, :step],\n",
    "        )\n",
    "\n",
    "        # cand_bbsz_idx contains beam indices for the top candidate\n",
    "        # hypotheses, with a range of values: [0, bsz*beam_size),\n",
    "        # and dimensions: [bsz, cand_size]\n",
    "        cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n",
    "\n",
    "        # finalize hypotheses that end in eos\n",
    "        # Shape of eos_mask: (batch size, beam size)\n",
    "        eos_mask = cand_indices.eq(dec_generator.eos) & cand_scores.ne(-math.inf)\n",
    "        eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0).to(eos_mask)\n",
    "\n",
    "        # only consider eos when it's among the top beam_size indices\n",
    "        # Now we know what beam item(s) to finish\n",
    "        # Shape: 1d list of absolute-numbered\n",
    "        eos_bbsz_idx = torch.masked_select(\n",
    "            cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]\n",
    "        )\n",
    "\n",
    "        finalized_sents: List[int] = []\n",
    "        if eos_bbsz_idx.numel() > 0:\n",
    "            eos_scores = torch.masked_select(\n",
    "                cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]\n",
    "            )\n",
    "\n",
    "            finalized_sents = dec_generator.finalize_hypos(\n",
    "                step,\n",
    "                eos_bbsz_idx,\n",
    "                eos_scores,\n",
    "                tokens,\n",
    "                scores,\n",
    "                finalized,\n",
    "                finished,\n",
    "                beam_size,\n",
    "                attn,\n",
    "                src_lengths,\n",
    "                max_len,\n",
    "            )\n",
    "            num_remaining_sent -= len(finalized_sents)\n",
    "\n",
    "        assert num_remaining_sent >= 0\n",
    "        if num_remaining_sent == 0:\n",
    "            break\n",
    "        assert step < max_len\n",
    "\n",
    "        # Remove finalized sentences (ones for which {beam_size}\n",
    "        # finished hypotheses have been generated) from the batch.\n",
    "        if len(finalized_sents) > 0:\n",
    "            new_bsz = bsz - len(finalized_sents)\n",
    "\n",
    "            # construct batch_idxs which holds indices of batches to keep for the next pass\n",
    "            batch_mask = torch.ones(bsz, dtype=torch.bool, device=cand_indices.device)\n",
    "            batch_mask[finalized_sents] = False\n",
    "            # TODO replace `nonzero(as_tuple=False)` after TorchScript supports it\n",
    "            batch_idxs = torch.arange(bsz, device=cand_indices.device).masked_select(batch_mask)\n",
    "\n",
    "            # Choose the subset of the hypothesized constraints that will continue\n",
    "            dec_generator.search.prune_sentences(batch_idxs)\n",
    "\n",
    "            eos_mask = eos_mask[batch_idxs]\n",
    "            cand_beams = cand_beams[batch_idxs]\n",
    "            bbsz_offsets.resize_(new_bsz, 1)\n",
    "            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n",
    "            cand_scores = cand_scores[batch_idxs]\n",
    "            cand_indices = cand_indices[batch_idxs]\n",
    "\n",
    "            if prefix_tokens is not None:\n",
    "                prefix_tokens = prefix_tokens[batch_idxs]\n",
    "            src_lengths = src_lengths[batch_idxs]\n",
    "            cands_to_ignore = cands_to_ignore[batch_idxs]\n",
    "\n",
    "            scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n",
    "            tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n",
    "            if attn is not None:\n",
    "                attn = attn.view(bsz, -1)[batch_idxs].view(\n",
    "                    new_bsz * beam_size, attn.size(1), -1\n",
    "                )\n",
    "            bsz = new_bsz\n",
    "        else:\n",
    "            batch_idxs = None\n",
    "\n",
    "        # Set active_mask so that values > cand_size indicate eos hypos\n",
    "        # and values < cand_size indicate candidate active hypos.\n",
    "        # After, the min values per row are the top candidate active hypos\n",
    "\n",
    "        # Rewrite the operator since the element wise or is not supported in torchscript.\n",
    "\n",
    "        eos_mask[:, :beam_size] = ~((~cands_to_ignore) & (~eos_mask[:, :beam_size]))\n",
    "        active_mask = torch.add(\n",
    "            eos_mask.type_as(cand_offsets) * cand_size,\n",
    "            cand_offsets[: eos_mask.size(1)],\n",
    "        )\n",
    "\n",
    "        # get the top beam_size active hypotheses, which are just\n",
    "        # the hypos with the smallest values in active_mask.\n",
    "        # {active_hypos} indicates which {beam_size} hypotheses\n",
    "        # from the list of {2 * beam_size} candidates were\n",
    "        # selected. Shapes: (batch size, beam size)\n",
    "        new_cands_to_ignore, active_hypos = torch.topk(\n",
    "            active_mask, k=beam_size, dim=1, largest=False\n",
    "        )\n",
    "\n",
    "        # update cands_to_ignore to ignore any finalized hypos.\n",
    "        cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n",
    "        # Make sure there is at least one active item for each sentence in the batch.\n",
    "        assert (~cands_to_ignore).any(dim=1).all()\n",
    "\n",
    "        # update cands_to_ignore to ignore any finalized hypos\n",
    "\n",
    "        # {active_bbsz_idx} denotes which beam number is continued for each new hypothesis (a beam\n",
    "        # can be selected more than once).\n",
    "        active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)\n",
    "        active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)\n",
    "\n",
    "        active_bbsz_idx = active_bbsz_idx.view(-1)\n",
    "        active_scores = active_scores.view(-1)\n",
    "\n",
    "        # copy tokens and scores for active hypotheses\n",
    "\n",
    "        # Set the tokens for each beam (can select the same row more than once)\n",
    "        tokens[:, : step + 1] = torch.index_select(\n",
    "            tokens[:, : step + 1], dim=0, index=active_bbsz_idx\n",
    "        )\n",
    "        # Select the next token for each of them\n",
    "        tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(\n",
    "            cand_indices, dim=1, index=active_hypos\n",
    "        )\n",
    "        if step > 0:\n",
    "            scores[:, :step] = torch.index_select(\n",
    "                scores[:, :step], dim=0, index=active_bbsz_idx\n",
    "            )\n",
    "        scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(\n",
    "            cand_scores, dim=1, index=active_hypos\n",
    "        )\n",
    "\n",
    "        # Update constraints based on which candidates were selected for the next beam\n",
    "        dec_generator.search.update_constraints(active_hypos)\n",
    "\n",
    "        # copy attention for active hypotheses\n",
    "        if attn is not None:\n",
    "            attn[:, :, : step + 2] = torch.index_select(\n",
    "                attn[:, :, : step + 2], dim=0, index=active_bbsz_idx\n",
    "            )\n",
    "\n",
    "        # reorder incremental state in decoder\n",
    "        reorder_state = active_bbsz_idx\n",
    "\n",
    "    # sort by score descending\n",
    "    for sent in range(len(finalized)):\n",
    "        scores = torch.tensor([float(elem[\"score\"].item()) for elem in finalized[sent]])\n",
    "        _, sorted_scores_indices = torch.sort(scores, descending=True)\n",
    "        finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]\n",
    "        finalized[sent] = torch.jit.annotate(List[Dict[str, Tensor]], finalized[sent])\n",
    "    return encoder_outs, decoder_outs, finalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intensive-hours",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gram_linear(x):\n",
    "  \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "  Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "\n",
    "  Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "  \"\"\"\n",
    "  return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "  \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "  Args:\n",
    "    x: A num_examples x num_features matrix of features.\n",
    "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "      bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "      possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "  Returns:\n",
    "    A num_examples x num_examples Gram matrix of examples.\n",
    "  \"\"\"\n",
    "  dot_products = x.dot(x.T)\n",
    "  sq_norms = np.diag(dot_products)\n",
    "  sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "  sq_median_distance = np.median(sq_distances)\n",
    "  return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "  \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "  This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "  induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "  Args:\n",
    "    gram: A num_examples x num_examples symmetric matrix.\n",
    "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "      estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "  Returns:\n",
    "    A symmetric matrix with centered columns and rows.\n",
    "  \"\"\"\n",
    "  if not np.allclose(gram, gram.T):\n",
    "    raise ValueError('Input must be a symmetric matrix.')\n",
    "  gram = gram.copy()\n",
    "\n",
    "  if unbiased:\n",
    "    # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "    # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "    # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "    # stable than the alternative from Song et al. (2007).\n",
    "    n = gram.shape[0]\n",
    "    np.fill_diagonal(gram, 0)\n",
    "    means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "    means -= np.sum(means) / (2 * (n - 1))\n",
    "    gram -= means[:, None]\n",
    "    gram -= means[None, :]\n",
    "    np.fill_diagonal(gram, 0)\n",
    "  else:\n",
    "    means = np.mean(gram, 0, dtype=np.float64)\n",
    "    means -= np.mean(means) / 2\n",
    "    gram -= means[:, None]\n",
    "    gram -= means[None, :]\n",
    "\n",
    "  return gram\n",
    "\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "  \"\"\"Compute CKA.\n",
    "\n",
    "  Args:\n",
    "    gram_x: A num_examples x num_examples Gram matrix.\n",
    "    gram_y: A num_examples x num_examples Gram matrix.\n",
    "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "  Returns:\n",
    "    The value of CKA between X and Y.\n",
    "  \"\"\"\n",
    "  gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "  gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "  # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "  # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "  scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "  normalization_x = np.linalg.norm(gram_x)\n",
    "  normalization_y = np.linalg.norm(gram_y)\n",
    "  return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(\n",
    "    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,\n",
    "    n):\n",
    "  \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "  # This formula can be derived by manipulating the unbiased estimator from\n",
    "  # Song et al. (2007).\n",
    "  return (\n",
    "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "  \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "  This is typically faster than computing the Gram matrix when there are fewer\n",
    "  features than examples.\n",
    "\n",
    "  Args:\n",
    "    features_x: A num_examples x num_features matrix of features.\n",
    "    features_y: A num_examples x num_features matrix of features.\n",
    "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "      biased. Note that this estimator may be negative.\n",
    "\n",
    "  Returns:\n",
    "    The value of CKA between X and Y.\n",
    "  \"\"\"\n",
    "  features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "  features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "  dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "  normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "  normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "  if debiased:\n",
    "    n = features_x.shape[0]\n",
    "    # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "    sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "    sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "    squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "    squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "    dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "        dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "        squared_norm_x, squared_norm_y, n)\n",
    "    normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "        normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "        squared_norm_x, squared_norm_x, n))\n",
    "    normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "        normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "        squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "  return dot_product_similarity / (normalization_x * normalization_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upset-career",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    data = torch.load(path)\n",
    "    return data[\"model\"]\n",
    "\n",
    "def load_data(path):\n",
    "    data = torch.load(path)\n",
    "    return data\n",
    "\n",
    "def load_dict(path: str) -> Dictionary:\n",
    "    d = Dictionary.load(path)\n",
    "    # for l in langs:\n",
    "    d.add_symbol(\"<mask>\")\n",
    "    return d\n",
    "\n",
    "def sent_to_ids(d, sent):\n",
    "    tokens = sent.split()\n",
    "    return tokens_to_ids(d, tokens)\n",
    "\n",
    "def dec_cal_mean(bart, d, sent):\n",
    "    token_ids = torch.tensor(sent_to_ids(d, sent), device=DEVICE)\n",
    "    all_layers = bart.extract_features(token_ids, return_all_hiddens=True)\n",
    "    return [x[0].mean(dim=0).tolist() for x in all_layers]\n",
    "\n",
    "def enc_cal_mean(bart, d, sent):\n",
    "    token_ids = torch.tensor([sent_to_ids(d, sent).tolist()], device=DEVICE)\n",
    "    encoder_layers = bart.model.encoder(token_ids,  None, return_all_hiddens=True)\n",
    "    embedding = encoder_layers.encoder_embedding\n",
    "    encoder_out = encoder_layers.encoder_out\n",
    "    all_layers = encoder_layers.encoder_states\n",
    "    \n",
    "    mean_layers = [torch.squeeze(embedding).mean(dim=0).tolist()]\n",
    "    for x in all_layers:\n",
    "        mean_layers.append(torch.squeeze(x).mean(dim=0).tolist())\n",
    "    mean_layers.append(torch.squeeze(encoder_out).mean(dim=0).tolist())\n",
    "        \n",
    "    return mean_layers\n",
    "\n",
    "def linear_HSIC(X, Y):\n",
    "    L_X = np.dot(X, X.T)\n",
    "    L_Y = np.dot(Y, Y.T)\n",
    "    return np.sum(centering(L_X) * centering(L_Y))\n",
    "\n",
    "def centering(K):\n",
    "    n = K.shape[0]\n",
    "    unit = np.ones([n, n])\n",
    "    I = np.eye(n)\n",
    "    H = I - unit / n\n",
    "\n",
    "    return np.dot(np.dot(H, K), H)\n",
    "\n",
    "def linear_CKA(X, Y):\n",
    "    hsic = linear_HSIC(X, Y)\n",
    "    var1 = np.sqrt(linear_HSIC(X, X))\n",
    "    var2 = np.sqrt(linear_HSIC(Y, Y))\n",
    "\n",
    "    return hsic / (var1 * var2)\n",
    "\n",
    "def cal_cka_sim(matrix1, matrix2):\n",
    "    dot = np.dot(matrix2.T, matrix1)\n",
    "    LA_norm = LA.norm(dot) ** 2\n",
    "    norm1 = LA.norm(np.dot(matrix1.T, matrix1))\n",
    "    norm2 = LA.norm(np.dot(matrix2.T, matrix2))\n",
    "    return LA_norm / (norm1 * norm2)\n",
    "\n",
    "def enc_cka_sim(jp_bart, ft, d, sentences):\n",
    "    jp_bart_d = defaultdict(list)\n",
    "    ft_d = defaultdict(list)\n",
    "    \n",
    "    for n, sent in enumerate(sentences):\n",
    "        \n",
    "        if (n+1) % 200 == 0:\n",
    "            print(f\"done {n+1}\")\n",
    "        sent = sent.strip()\n",
    "        jp_bart_vecs = enc_cal_mean(bart=jp_bart, d=d, sent=sent)\n",
    "        ft_vecs = enc_cal_mean(bart=ft, d=d, sent=sent)\n",
    "        \n",
    "        for n, (jp_vec, ft_vec) in enumerate(zip(jp_bart_vecs, ft_vecs)):\n",
    "            jp_bart_d[n].append(jp_vec)\n",
    "            ft_d[n].append(ft_vec)\n",
    "    \n",
    "    for key in jp_bart_d.keys():\n",
    "        jp_bart_matrix = np.array(jp_bart_d[key])\n",
    "        ft_matrix = np.array(ft_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"v2\", key, linear_CKA(jp_bart_matrix, ft_matrix))\n",
    "        \n",
    "    \n",
    "def dec_cka_sim(jp_bart, ft, d, sentences):\n",
    "    jp_bart_d = defaultdict(list)\n",
    "    ft_d = defaultdict(list)\n",
    "    \n",
    "    for n, sent in enumerate(sentences):\n",
    "        if (n+1) % 200 == 0:\n",
    "            print(f\"done {n+1}\")\n",
    "        sent = sent.strip()\n",
    "        jp_bart_vecs = dec_cal_mean(bart=jp_bart, d=d, sent=sent)\n",
    "        ft_vecs = dec_cal_mean(bart=ft, d=d, sent=sent)\n",
    "        \n",
    "        for n, (jp_vec, ft_vec) in enumerate(zip(jp_bart_vecs, ft_vecs)):\n",
    "            jp_bart_d[n].append(jp_vec)\n",
    "            ft_d[n].append(ft_vec)\n",
    "    \n",
    "    for key in jp_bart_d.keys():\n",
    "        jp_bart_matrix = np.array(jp_bart_d[key])\n",
    "        ft_matrix = np.array(ft_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"v2\", key, linear_CKA(jp_bart_matrix, ft_matrix))\n",
    "\n",
    "def tokens_to_ids(d, tokens):\n",
    "    idxs = []\n",
    "    for token in tokens:\n",
    "        idx = d.index(token)\n",
    "        idxs.append(idx)\n",
    "    return torch.tensor(idxs)\n",
    "\n",
    "def ids_to_tokens(d, idxs):\n",
    "    tokens = []\n",
    "    for idx in idxs:\n",
    "        token = d[idx]\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def load_bart(path, model_name):\n",
    "    bart = BARTModel.from_pretrained(path, checkpoint_file=model_name)\n",
    "    bart.eval()\n",
    "    return bart\n",
    "\n",
    "import copy\n",
    "\n",
    "def generate(tokens, model, beam: int = 1,  **kwargs):\n",
    "    sample = build_sample(tokens)\n",
    "    # build dec_generator using current args as well as any kwargs\n",
    "    gen_args = copy.copy(model.args)\n",
    "    gen_args.beam = beam\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(gen_args, k, v)\n",
    "    generator = model.task.build_generator([model.model], gen_args)\n",
    "    encoder_outs, decoder_outs, translations = _generate(\n",
    "        generator,\n",
    "        sample,\n",
    "        prefix_tokens=sample['net_input']['src_tokens'].new_zeros((len(tokens), 1)).fill_(model.task.source_dictionary.bos()),\n",
    "    )\n",
    "    \n",
    "    def getarg(name, default):\n",
    "        return getattr(gen_args, name, getattr(model.args, name, default))\n",
    "\n",
    "    # Process top predictions\n",
    "    hypos = [x[0] for x in translations]\n",
    "    hypos = [v for _, v in sorted(zip(sample['id'].tolist(), hypos))]\n",
    "    return encoder_outs, decoder_outs, hypos\n",
    "\n",
    "def generate_unsupervised(tokens, enc_model, dec_model, beam: int = 1,  **kwargs):\n",
    "    sample = build_sample(tokens)\n",
    "    # build generator using current args as well as any kwargs\n",
    "    enc_gen_args = copy.copy(enc_model.args)\n",
    "    enc_gen_args.beam = beam\n",
    "    \n",
    "    dec_gen_args = copy.copy(dec_model.args)\n",
    "    dec_gen_args.beam = beam\n",
    "    \n",
    "    for k, v in kwargs.items():\n",
    "        setattr(gen_args, k, v)\n",
    "    enc_generator = enc_model.task.build_generator([enc_model.model], enc_gen_args)\n",
    "    dec_generator = dec_model.task.build_generator([dec_model.model], dec_gen_args)\n",
    "    encoder_outs, decoder_outs, translations = _generate_unsupervised(\n",
    "        enc_generator,\n",
    "        dec_generator,\n",
    "        sample,\n",
    "        prefix_tokens=sample['net_input']['src_tokens'].new_zeros((len(tokens), 1)).fill_(dec_model.task.source_dictionary.bos()),\n",
    "    )\n",
    "    \n",
    "    def getarg(name, default):\n",
    "        return getattr(gen_args, name, getattr(model.args, name, default))\n",
    "\n",
    "    # Process top predictions\n",
    "#     print(translations)\n",
    "    hypos = [x[0] for x in translations]\n",
    "    hypos = [v for _, v in sorted(zip(sample['id'].tolist(), hypos))]\n",
    "    return encoder_outs, decoder_outs, hypos\n",
    "\n",
    "def build_sample(tokens):\n",
    "    d = {'id': torch.tensor([0]),\n",
    "         'nsentences': len(tokens),\n",
    "         'ntokens': len(tokens[0]),\n",
    "         'net_input': {'src_tokens': tokens,\n",
    "                      'src_lengths': torch.tensor(len(tokens[0]))},\n",
    "         'target': None}\n",
    "    return d\n",
    "\n",
    "def encdec_cal_mean(model, d, sent):\n",
    "    token_ids = torch.tensor([sent_to_ids(d, sent).tolist()], device=DEVICE)\n",
    "    encoder_outs, decoder_outs, outs = generate(token_ids, model)\n",
    "    \n",
    "    enc_mean_vecs = enc_mean(encoder_outs[0])\n",
    "    inner_mean_layers, self_attn_mean_layers = dec_mean(decoder_outs)\n",
    "    \n",
    "    return enc_mean_vecs, {\"inner_mean_layers\": inner_mean_layers, \"self_attn_mean_layers\": self_attn_mean_layers}\n",
    "\n",
    "def encdec_cal_mean_v2(model, d, sent):\n",
    "    token_ids = torch.tensor([sent_to_ids(d, sent).tolist()], device=DEVICE)\n",
    "    encoder_outs, decoder_outs, outs = generate(token_ids, model)\n",
    "    \n",
    "    enc_mean_vecs = enc_mean_v2(encoder_outs[0])\n",
    "    dec_mean_vecs = dec_mean_v2(decoder_outs)\n",
    "    \n",
    "    return enc_mean_vecs, dec_mean_vecs\n",
    "    \n",
    "def enc_mean(encoder_layers):\n",
    "    embedding = encoder_layers.encoder_embedding\n",
    "    encoder_out = encoder_layers.encoder_out\n",
    "    all_layers = encoder_layers.encoder_states\n",
    "    \n",
    "    mean_layers = [torch.squeeze(embedding).mean(dim=0).tolist()]\n",
    "    for x in all_layers:\n",
    "        mean_layers.append(torch.squeeze(x).mean(dim=0).tolist())\n",
    "    mean_layers.append(torch.squeeze(encoder_out).mean(dim=0).tolist())\n",
    "        \n",
    "    return mean_layers\n",
    "\n",
    "def enc_mean_v2(encoder_layers):\n",
    "    embedding = encoder_layers.encoder_embedding\n",
    "    encoder_out = encoder_layers.encoder_out\n",
    "    all_layers = encoder_layers.encoder_states\n",
    "    \n",
    "    mean_layers = [torch.squeeze(embedding).mean(dim=0).tolist()[1:-1]]\n",
    "    for x in all_layers:\n",
    "        mean_layers.append(torch.squeeze(x).mean(dim=0).tolist()[1:-1])\n",
    "    mean_layers.append(torch.squeeze(encoder_out).mean(dim=0).tolist()[1:-1])\n",
    "        \n",
    "    return mean_layers\n",
    "\n",
    "def dec_mean(dec_outs):\n",
    "    inner_mean_layers = []\n",
    "    self_attn_mean_layers = []\n",
    "    \n",
    "    for layer in range(len(dec_outs[0][1][\"inner_states\"])):\n",
    "        tensors = [x[1][\"inner_states\"][layer][0][0].tolist() for x in dec_outs]\n",
    "        inner_mean_layers.append(torch.tensor(tensors).mean(dim=0).tolist())\n",
    "        \n",
    "    for layer in range(len(dec_outs[0][1][\"self_dec_attns\"])):\n",
    "        tensors = [x[1][\"self_dec_attns\"][layer][0][0].tolist() for x in dec_outs]\n",
    "        self_attn_mean_layers.append(torch.tensor(tensors).mean(dim=0).tolist())\n",
    "        \n",
    "    return inner_mean_layers, self_attn_mean_layers\n",
    "\n",
    "def dec_mean_v2(dec_outs):\n",
    "    mean_layers = []\n",
    "    \n",
    "    for layer in range(len(dec_outs[0][1][\"inner_states\"])):\n",
    "        tensors = [x[1][\"inner_states\"][layer][0][0].tolist() for x in dec_outs]\n",
    "        mean_layers.append(torch.tensor(tensors).mean(dim=0).tolist()[1:])\n",
    "    \n",
    "    return mean_layers\n",
    "\n",
    "def encdec_cka_sim(pre, ft, pre_d, ft_d, pre_sentences, ft_sentences):\n",
    "    pre_enc_d = defaultdict(list)\n",
    "    pre_dec_d = defaultdict(list)\n",
    "    pre_dec_self_attn_d = defaultdict(list)\n",
    "    \n",
    "    ft_dec_d = defaultdict(list)\n",
    "    ft_enc_d = defaultdict(list)\n",
    "    ft_dec_self_attn_d = defaultdict(list)\n",
    "    \n",
    "    for n, (pre_sent, ft_sent) in enumerate(zip(pre_sentences, ft_sentences)):\n",
    "        if (n+1) % 200 == 0:\n",
    "            print(f\"done {n+1}\")\n",
    "        pre_sent = pre_sent.strip()\n",
    "        ft_sent = ft_sent.strip()\n",
    "        pre_enc_vecs, pre_dec_vecs = encdec_cal_mean(model=pre, d=pre_d, sent=pre_sent)\n",
    "        ft_enc_vecs, ft_dec_vecs = encdec_cal_mean(model=ft, d=ft_d, sent=ft_sent)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_enc_vecs, ft_enc_vecs)):\n",
    "            pre_enc_d[n].append(pre_vec)\n",
    "            ft_enc_d[n].append(ft_vec)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_dec_vecs[\"inner_mean_layers\"], \\\n",
    "                                                  ft_dec_vecs[\"inner_mean_layers\"])):\n",
    "            pre_dec_d[n].append(pre_vec)\n",
    "            ft_dec_d[n].append(ft_vec)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_dec_vecs[\"self_attn_mean_layers\"], \\\n",
    "                                                  ft_dec_vecs[\"self_attn_mean_layers\"])):\n",
    "            pre_dec_self_attn_d[n].append(pre_vec)\n",
    "            ft_dec_self_attn_d[n].append(ft_vec)\n",
    "    \n",
    "    print(\"Encoder CKA\")\n",
    "    for key in pre_enc_d.keys():\n",
    "        pre_matrix = np.array(pre_enc_d[key])\n",
    "        ft_matrix = np.array(ft_enc_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "    \n",
    "    print(\"\\nDecoder CKA\")\n",
    "    for key in pre_dec_d.keys():\n",
    "        pre_matrix = np.array(pre_dec_d[key])\n",
    "        ft_matrix = np.array(ft_dec_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "    \n",
    "    print(\"\\nDecoder Self Attention CKA\")\n",
    "    for key in pre_dec_self_attn_d.keys():\n",
    "        pre_matrix = np.array(pre_dec_self_attn_d[key])\n",
    "        ft_matrix = np.array(ft_dec_self_attn_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "\n",
    "\n",
    "def encdec_cka_sim_rm_special_tokens(pre, ft, pre_d, ft_d, pre_sentences, ft_sentences, debiased=False):\n",
    "    pre_enc_d = defaultdict(list)\n",
    "    pre_dec_d = defaultdict(list)\n",
    "    \n",
    "    ft_dec_d = defaultdict(list)\n",
    "    ft_enc_d = defaultdict(list)\n",
    "    \n",
    "    for n, (pre_sent, ft_sent) in enumerate(zip(pre_sentences, ft_sentences)):\n",
    "        if (n+1) % 200 == 0:\n",
    "            print(f\"done {n+1}\")\n",
    "        pre_sent = pre_sent.strip()\n",
    "        ft_sent = ft_sent.strip()\n",
    "        pre_enc_vecs, pre_dec_vecs = encdec_cal_mean_v2(model=pre, d=pre_d, sent=pre_sent)\n",
    "        ft_enc_vecs, ft_dec_vecs = encdec_cal_mean_v2(model=ft, d=ft_d, sent=ft_sent)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_enc_vecs, ft_enc_vecs)):\n",
    "            pre_enc_d[n].append(pre_vec)\n",
    "            ft_enc_d[n].append(ft_vec)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_dec_vecs, ft_dec_vecs)):\n",
    "            pre_dec_d[n].append(pre_vec)\n",
    "            ft_dec_d[n].append(ft_vec)\n",
    "    \n",
    "    print(\"Encoder CKA\")\n",
    "    for key in pre_enc_d.keys():\n",
    "        pre_matrix = np.array(pre_enc_d[key])\n",
    "        ft_matrix = np.array(ft_enc_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"Layer\", key, feature_space_linear_cka(pre_matrix, ft_matrix, debiased=debiased))\n",
    "    \n",
    "    print(\"\\nDecoder CKA\")\n",
    "    for key in pre_dec_d.keys():\n",
    "        pre_matrix = np.array(pre_dec_d[key])\n",
    "        ft_matrix = np.array(ft_dec_d[key])\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "        print(\"Layer\", key, feature_space_linear_cka(pre_matrix, ft_matrix, debiased=debiased))\n",
    "    \n",
    "def cal_layer_sim(ft, pre, embs, padding_masks):\n",
    "    \n",
    "    enc_d = defaultdict(list)\n",
    "    dec_d = defaultdict(list)\n",
    "    \n",
    "    for emb, padding_mask in zip(embs, padding_masks):\n",
    "        \n",
    "        for idx, (ft_layer, pre_layer) in enumerate(zip(ft.model.encoder.layers, pre.model.encoder.layers)):\n",
    "            ft_outs = ft_layer(emb, padding_mask)\n",
    "            pre_outs = pre_layer(emb, padding_mask)\n",
    "            enc_d[idx].append(cos(ft_outs[0], pre_outs[0]).tolist()[0])\n",
    "\n",
    "        for idx, (ft_layer, pre_layer) in enumerate(zip(ft.model.decoder.layers, \\\n",
    "                                                        pre.model.decoder.layers)):\n",
    "\n",
    "            ft_outs = ft_layer(\n",
    "                        emb,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        self_attn_mask=None,\n",
    "                        self_attn_padding_mask=padding_mask,\n",
    "                        need_attn=bool((idx==ft.model.decoder.num_layers-1)),\n",
    "                        need_head_weights=bool((idx==ft.model.decoder.num_layers-1))\n",
    "                      )\n",
    "\n",
    "            pre_outs = pre_layer(\n",
    "                        emb,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        self_attn_mask=None,\n",
    "                        self_attn_padding_mask=padding_mask,\n",
    "                        need_attn=bool((idx==pre.model.decoder.num_layers-1)),\n",
    "                        need_head_weights=bool((idx==pre.model.decoder.num_layers-1))\n",
    "                      )\n",
    "            \n",
    "#             print(cos(ft_outs[0], pre_outs[0]))\n",
    "            dec_d[idx].append(cos(ft_outs[0][0], pre_outs[0][0]).tolist()[0])\n",
    "            \n",
    "    print(\"Encoder\")\n",
    "    for key, values in enc_d.items():\n",
    "        print(key, sum(values)/len(values))\n",
    "    print(\"\\nDecoder\")\n",
    "    for key, values in dec_d.items():\n",
    "        print(key, sum(values)/len(values))\n",
    "\n",
    "        \n",
    "def fusion_model(model1_path, model2_path, save_path, dec_flag=False):\n",
    "    data1 = load_data(model1_path)\n",
    "    model1 = data1[\"model\"]\n",
    "    \n",
    "    data2 = load_data(model2_path)\n",
    "    model2 = data2[\"model\"]\n",
    "    \n",
    "    for key in model1.keys():\n",
    "        if key == 'encoder.layer_norm.weight' or key == 'encoder.layer_norm.bias':\n",
    "            print(key)\n",
    "            model1[key] = model2[key]\n",
    "#         if \"decoder.layers.0\" in key or \"decoder.layers.1\" in key or \\\n",
    "#            \"decoder.layers.2\" in key or \"decoder.layers.4\" in key:        \n",
    "#             model1[key] = model2[key]\n",
    "#         elif \"decoder.layers.5\" in key and dec_flag:\n",
    "#             print(key)\n",
    "#             model1[key] = model2[key]\n",
    "    \n",
    "#     data1[\"args\"].share_all_embeddings = False\n",
    "#     print(data1[\"args\"].share_all_embeddings)\n",
    "    torch.save(data1, save_path)\n",
    "\n",
    "    \n",
    "def fusion_enc_dec(enc_path, dec_path, save_path):\n",
    "    enc = load_data(enc_path)\n",
    "    enc_model = enc[\"model\"]\n",
    "    \n",
    "    dec = load_data(dec_path)\n",
    "    dec_model = dec[\"model\"]\n",
    "    \n",
    "    enc_keys = list(enc_model.keys())\n",
    "        \n",
    "    for key in enc_keys:\n",
    "        if key not in dec_model:\n",
    "            enc_model.pop(key)\n",
    "            continue\n",
    "        elif \"encoder.\" in key:\n",
    "            continue\n",
    "            \n",
    "        enc_model[key] = dec_model[key]\n",
    "    \n",
    "    enc[\"args\"].share_all_embeddings = False\n",
    "#     print(data1[\"args\"].share_all_embeddings)\n",
    "    torch.save(enc, save_path)\n",
    "    \n",
    "    \n",
    "def initialize_encoder_only(model_path, save_path, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    data = load_data(model_path)\n",
    "    model = data[\"model\"]\n",
    "\n",
    "    for key in model.keys():\n",
    "        if 'encoder.' in key or 'emb' in key \\\n",
    "            or key == \"decoder.output_projection.weight\" or \".version\" in key:\n",
    "            continue\n",
    "        \n",
    "        matrix = model[key].to('cpu').detach().numpy().copy()\n",
    "        std = matrix.std()\n",
    "        mean = matrix.mean()\n",
    "        shape = matrix.shape\n",
    "#         print(key, mean, std)\n",
    "        model[key] = torch.normal(\n",
    "            mean, std, size=shape, dtype=model[key].dtype, layout=model[key].layout, device=model[key].device\n",
    "        )\n",
    "        \n",
    "    torch.save(data, save_path)\n",
    "\n",
    "def initialize_decoder_only(model_path, save_path, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    data = load_data(model_path)\n",
    "    model = data[\"model\"]\n",
    "\n",
    "    for key in model.keys():\n",
    "        if 'decoder.' in key or 'emb' in key \\\n",
    "            or key == \"decoder.output_projection.weight\" or \".version\" in key:\n",
    "            continue\n",
    "        \n",
    "        matrix = model[key].to('cpu').detach().numpy().copy()\n",
    "        std = matrix.std()\n",
    "        mean = matrix.mean()\n",
    "        shape = matrix.shape\n",
    "#         print(key, mean, std)\n",
    "        model[key] = torch.normal(\n",
    "            mean, std, size=shape, dtype=model[key].dtype, layout=model[key].layout, device=model[key].device\n",
    "        )\n",
    "        \n",
    "    torch.save(data, save_path)\n",
    "\n",
    "def sents_to_encodervecs(model, sents, d, layer=-1):\n",
    "    vecs = []\n",
    "    for sent in sents:\n",
    "        token_ids = torch.tensor([sent_to_ids(d=d, sent=sent).tolist()], device=DEVICE)\n",
    "        enc_outs = model.model.encoder(token_ids,  None, return_all_hiddens=True)\n",
    "        enc_vecs = enc_mean(enc_outs)\n",
    "        vecs.append(enc_vecs[layer])\n",
    "    return vecs\n",
    "\n",
    "def precision_topk(preds, topk):\n",
    "    num = 0\n",
    "    for n, x in enumerate(preds):\n",
    "        if n in x[:topk]:\n",
    "            num += 1\n",
    "    return num\n",
    "\n",
    "def sent_similarity_topk(model1, model2, sents1, sents2, d1, d2):\n",
    "    length = len(sents1)\n",
    "    \n",
    "    sent_vecs1 = torch.tensor(sents_to_encodervecs(model=model1, sents=sents1, d=d1))\n",
    "    sent_vecs2 = torch.tensor(sents_to_encodervecs(model=model2, sents=sents2, d=d2))\n",
    "    print(\"done cal vecs\")\n",
    "#     print(sent_vecs2)\n",
    "    sims = [cos(sent_vecs1[n:n+1], sent_vecs2).to('cpu').detach().numpy().copy() for n in range(len(sent_vecs1))]\n",
    "    print(sims)\n",
    "    preds = [np.argsort(x)[::-1] for x in sims]\n",
    "    \n",
    "    topk = 1\n",
    "    num = precision_topk(preds=preds, topk=topk)\n",
    "    print(topk, num, num/length)\n",
    "    \n",
    "    topk = 10\n",
    "    num = precision_topk(preds=preds, topk=topk)\n",
    "    print(topk, num, num/length)\n",
    "\n",
    "# nmt -> layers\n",
    "# bert -> layer\n",
    "def fusion_bertenc(bert_path, dec_path, save_path):\n",
    "    bert = torch.load(bert_path)\n",
    "    \n",
    "    dec = load_data(dec_path)\n",
    "    dec_model = dec[\"model\"]\n",
    "    \n",
    "    # bert embs -> dec_model\n",
    "    key = \"bert.embeddings.word_embeddings.weight\"\n",
    "    dec_model[\"encoder.embed_tokens.weight\"] = bert[key]\n",
    "    dec_model[\"decoder.embed_tokens.weight\"] = bert[key]\n",
    "    dec_model[\"decoder.output_projection.weight\"] = bert[key]\n",
    "    print(dec_model[\"decoder.output_projection.weight\"].shape)\n",
    "    print(dec_model[\"decoder.embed_tokens.weight\"].shape)\n",
    "    print(dec_model[\"decoder.output_projection.weight\"].shape)\n",
    "    \n",
    "    # bert layers -> dec_model\n",
    "    for layer in range(12):\n",
    "        bert_keys = [x for x in bert.keys() if f\"layer.{layer}\" in x]\n",
    "        dec_keys = [x for x in dec_model.keys() if f\"encoder.layers.{layer}\" in x]\n",
    "        for bert_key, dec_key in zip(bert_keys, dec_keys):\n",
    "            dec_model[dec_key] = bert[bert_key]\n",
    "    \n",
    "    dec_keys = [x for x in dec_model.keys() if \"decoder.layers\" in x]\n",
    "    for key in dec_keys:\n",
    "        matrix = dec_model[key].to('cpu').detach().numpy().copy()\n",
    "        std = matrix.std()\n",
    "        mean = matrix.mean()\n",
    "        shape = matrix.shape\n",
    "#         print(key, mean, std)\n",
    "        dec_model[key] = torch.normal(\n",
    "            mean, std, size=shape, \n",
    "            dtype=dec_model[key].dtype, \n",
    "            layout=dec_model[key].layout, \n",
    "            device=dec_model[key].device\n",
    "        )\n",
    "    \n",
    "#     enc[\"args\"].share_all_embeddings = False\n",
    "#     print(data1[\"args\"].share_all_embeddings)\n",
    "    torch.save(dec, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charming-knife",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b67cf1b3ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbert_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../pretrained_bart/trim/koja_trimed_bert.bin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdec_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../ja-ko/bert/base/checkpoints/checkpoint1.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../pretrained_bart/trim/jabert_enc.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-7-6108a78b1adf>\u001b[0m in \u001b[0;36mfusion_bertenc\u001b[0;34m(bert_path, dec_path, save_path)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;31m# bert -> layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfusion_bertenc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fusion_bertenc(\n",
    "    bert_path=\"../pretrained_bart/trim/koja_trimed_bert.bin\", \n",
    "    dec_path=\"../ja-ko/bert/base/checkpoints/checkpoint1.pt\", \n",
    "    save_path=\"../pretrained_bart/trim/jabert_enc.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ordered-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trim = torch.load(\"../pretrained_bart/trim/koja_trimed_bert.bin\")\n",
    "bert = torch.load(\"../pretrained_bart/Japanese_L-12_H-768_A-12_E-30_BPE_WWM/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "committed-reservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 10\n",
    "bert[\"bert.embeddings.word_embeddings.weight\"][1575].tolist() == bert_trim[\"bert.embeddings.word_embeddings.weight\"][108].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "purple-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_enc_dec(\n",
    "    enc_path=\"../pretrained_bart/trim/jaen_ja_bart_base.pt\", \n",
    "    dec_path=\"../pretrained_bart/trim/jaen_en_bart_base.pt\", \n",
    "    save_path=\"../pretrained_bart/trim/jaenc_endec_bart.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "inappropriate-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_enc_dec(\n",
    "    enc_path=\"../pretrained_bart/trim/jaen_en_bart_base.pt\", \n",
    "    dec_path=\"../pretrained_bart/trim/jaen_ja_bart_base.pt\", \n",
    "    save_path=\"../pretrained_bart/trim/enenc_jadec_bart.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "shared-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_decoder = load_model(\"../../jpBART/japanese_bart_base_1.1/test/init_decoder.pt\")\n",
    "# init_encoder = load_model(\"../../jpBART/japanese_bart_base_1.1/test/init_encoder.pt\")\n",
    "ja_model = load_model(\"../pretrained_bart/trim/jaen_ja_bart_base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "solar-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = load_model(\"../pretrained_bart/trim/jaen_en_bart_base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "searching-karen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "yellow-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_jaen = load_model(\"../pretrained_bart/trim/jaenc_endec_bart.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "satisfactory-physiology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in fusion_jaen.keys() if x not in en_model.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "automatic-matter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.layer_norm.weight',\n",
       " 'encoder.layer_norm.bias',\n",
       " 'decoder.layer_norm.weight',\n",
       " 'decoder.layer_norm.bias',\n",
       " 'decoder.output_projection.weight']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in base_model.keys() if x not in en_model.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "scheduled-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_encoder_only(\n",
    "    model_path=\"../../jpBART/japanese_bart_base_1.1/test/model.pt\", \\\n",
    "    save_path=\"../../jpBART/japanese_bart_base_1.1/test/init_encoder.pt\", \\\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "embedded-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_decoder_only(\n",
    "    model_path=\"../../jpBART/japanese_bart_base_1.1/test/model.pt\", \\\n",
    "    save_path=\"../../jpBART/japanese_bart_base_1.1/test/init_decoder.pt\", \\\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "recognized-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_encoder_only(\n",
    "    model_path=\"../../jpBART/en-ja/ja-bart/model.pt\", \\\n",
    "    save_path=\"../../jpBART/en-ja/ja-bart/init_encoder.pt\", \\\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "realistic-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_decoder_only(\n",
    "    model_path=\"../../jpBART/en-ja/ja-bart/model.pt\", \\\n",
    "    save_path=\"../../jpBART/en-ja/ja-bart/init_decoder.pt\", \\\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suspected-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model(\"../../jpBART/japanese_bart_base_1.1/test/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "unknown-grammar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dec_outs[-1][1]['inner_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [x[1][\"inner_states\"][layer][0][0].tolist() for x in dec_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tensors).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-daily",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "million-complaint",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enBART and fine-tuned by jaen or enja\n",
    "enbart_ft_enja_path = \"../en-ja/enbart/checkpoints\"\n",
    "enbart_ft_enja_name = \"checkpoint_best.pt\"\n",
    "enbart_ft_enja = load_bart(path=enbart_ft_enja_path, model_name=enbart_ft_enja_name)\n",
    "enbart_ft_enja = enbart_ft_enja.to(DEVICE)\n",
    "\n",
    "enbart_ft_jaen_path = \"../ja-en/enbart/v2/checkpoints\"\n",
    "enbart_ft_jaen_name = \"checkpoint_last.pt\"\n",
    "enbart_ft_jaen = load_bart(path=enbart_ft_jaen_path, model_name=enbart_ft_jaen_name)\n",
    "enbart_ft_jaen = enbart_ft_jaen.to(DEVICE)\n",
    "\n",
    "enbart_enja_path = \"../pretrained_bart/trim/enbart_jaen\"\n",
    "enbart_enja_name = \"jaen_en_bart_base.pt\"\n",
    "enbart_enja = load_bart(path=enbart_enja_path, model_name=enbart_enja_name)\n",
    "enbart_enja = enbart_enja.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "combined-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enBART and fine-tuned by fren or enfr\n",
    "enbart_ft_enfr_path = \"../en-fr/bart/1M/checkpoints\"\n",
    "enbart_ft_enfr_name = \"checkpoint_best.pt\"\n",
    "enbart_ft_enfr = load_bart(path=enbart_ft_enfr_path, model_name=enbart_ft_enfr_name)\n",
    "enbart_ft_enfr = enbart_ft_enfr.to(DEVICE)\n",
    "\n",
    "enbart_ft_fren_path = \"../fr-en/bart/1M/v2/checkpoints\"\n",
    "enbart_ft_fren_name = \"checkpoint_best.pt\"\n",
    "enbart_ft_fren = load_bart(path=enbart_ft_fren_path, model_name=enbart_ft_fren_name)\n",
    "enbart_ft_fren = enbart_ft_fren.to(DEVICE)\n",
    "\n",
    "enbart_enfr_path = \"../pretrained_bart/trim\"\n",
    "enbart_enfr_name = \"enfr_enbart_random_sampling.pt\"\n",
    "enbart_enfr = load_bart(path=enbart_enfr_path, model_name=enbart_enfr_name)\n",
    "enbart_enfr = enbart_enfr.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "possible-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dict of ft enBART\n",
    "enja_enbart_d = load_dict(\"../pretrained_bart/trim/enbart_jaen/dict.txt\")\n",
    "enfr_enbart_d = load_dict(\"../pretrained_bart/trim/dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "constant-composer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ft_jako_path = \"../ja-ko/bart/checkpoints\"\n",
    "ft_jako_name = \"checkpoint_best.pt\"\n",
    "ft_jako= load_bart(path=ft_jako_path, model_name=ft_jako_name).to(DEVICE)\n",
    "\n",
    "ft_koja_path = \"../ko-ja/bart/checkpoints\"\n",
    "ft_koja_name = \"checkpoint_best.pt\"\n",
    "ft_koja = load_bart(path=ft_koja_path, model_name=ft_koja_name).to(DEVICE)\n",
    "\n",
    "ft_enja_path = \"../en-ja/bart/v2/checkpoints\"\n",
    "ft_enja_name = \"checkpoint_best.pt\"\n",
    "ft_enja = load_bart(path=ft_enja_path, model_name=ft_enja_name).to(DEVICE)\n",
    "\n",
    "ft_jaen_path = \"../ja-en/bart/checkpoints\"\n",
    "ft_jaen_name = \"checkpoint_best.pt\"\n",
    "ft_jaen = load_bart(path=ft_jaen_path, model_name=ft_jaen_name).to(DEVICE)\n",
    "\n",
    "jabart_jako_path = \"../pretrained_bart/trim/jabart_jako\"\n",
    "jabart_jako_name = \"ja_bart_base.pt\"\n",
    "jabart_jako = load_bart(path=jabart_jako_path, model_name=jabart_jako_name).to(DEVICE)\n",
    "\n",
    "# ft_jako_fa_path = \"../ja-ko/bart/fastalign/checkpoints\"\n",
    "# ft_jako_fa_name = \"checkpoint_best.pt\"\n",
    "# ft_jako_fa = load_bart(path=ft_jako_fa_path, model_name=ft_jako_fa_name).to(DEVICE)\n",
    "\n",
    "# ft_koja_fa_path = \"../ko-ja/bart/fastalign/checkpoints\"\n",
    "# ft_koja_fa_name = \"checkpoint_best.pt\"\n",
    "# ft_koja_fa = load_bart(path=ft_koja_fa_path, model_name=ft_koja_fa_name).to(DEVICE)\n",
    "\n",
    "# jabart_jako_fa_path = \"../pretrained_bart/muse/koja\"\n",
    "# jabart_jako_fa_name = \"fastalign_bart.pt\"\n",
    "# jabart_jako_fa = load_bart(path=jabart_jako_fa_path, model_name=jabart_jako_fa_name).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "delayed-reading",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dict of ft jaBART\n",
    "koja_d = load_dict(\"../pretrained_bart/trim/jabart_jako/dict.txt\")\n",
    "enja_d = load_dict(\"../en-ja/bart/v2/checkpoints/dict.en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-final",
   "metadata": {},
   "source": [
    "## Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "appreciated-studio",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enja sentences\n",
    "# file_path = \"../data/enja/enBART/dev.en\"\n",
    "# with open(file_path, \"r\") as f:\n",
    "#     en_enbart_sentences = f.readlines()\n",
    "    \n",
    "# file_path = \"../data/enja/enBART/dev.ja\"\n",
    "# with open(file_path, \"r\") as f:d\n",
    "#     ja_enbart_sentences = f.readlines()\n",
    "\n",
    "# file_path = \"../data/enja_v2/dev.en\"\n",
    "# with open(file_path, \"r\") as f:\n",
    "#     jaen_sentences_en = f.readlines()\n",
    "    \n",
    "# file_path = \"../data/enja_v2/dev.ja\"\n",
    "# with open(file_path, \"r\") as f:\n",
    "#     jaen_sentences_ja = f.readlines()\n",
    "    \n",
    "# enbart sentences\n",
    "file_path = \"../data/enja_2/enBART/dev.ja\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    enbart_jaen_sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/enja_2/enBART/dev.en\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    enbart_jaen_sentences_en = f.readlines()\n",
    "    \n",
    "file_path = \"../data/enfr/random/dev.fr\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    enbart_fren_sentences_fr = f.readlines()\n",
    "\n",
    "file_path = \"../data/enfr/random/dev.en\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    enbart_fren_sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incoming-million",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"../../dev_head10.sp.ko\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    ko_koen_setntences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "heated-stick",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# koja sentencef\n",
    "file_path = \"../data/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    jako_sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/dev.ko\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    jako_sentences_ko = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "breeding-chapel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "koenja_bart = load_bart(\"../koen-ja/trained\", \"bart.pt\").to(DEVICE)\n",
    "koenja_base = load_bart(\"../koen-ja/trained\", \"base.pt\").to(DEVICE)\n",
    "# koenja_d = load_dict(\"../koen-ja/trained/dict.ja.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "noticed-lecture",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.models.bart.hub_interface.BARTHubInterface"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(koenja_bart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-jackson",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentence similarity topk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-ottawa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Korean and Japanese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bored-mobility",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "[array([0.94447064, 0.8985309 , 0.77088374, 0.876096  , 0.91257375,\n",
      "       0.87748945, 0.8765159 , 0.8220344 , 0.9025764 , 0.8711979 ],\n",
      "      dtype=float32), array([0.90635896, 0.9177929 , 0.7812545 , 0.8878749 , 0.903947  ,\n",
      "       0.8812882 , 0.8821026 , 0.8397099 , 0.9103645 , 0.88529974],\n",
      "      dtype=float32), array([0.8803854 , 0.8578151 , 0.88624156, 0.90691185, 0.89150316,\n",
      "       0.8919578 , 0.8877258 , 0.8240379 , 0.87416893, 0.861958  ],\n",
      "      dtype=float32), array([0.9092634 , 0.8979603 , 0.8384323 , 0.9440382 , 0.91345626,\n",
      "       0.91033113, 0.88732105, 0.838293  , 0.9128608 , 0.87709403],\n",
      "      dtype=float32), array([0.9058174 , 0.8721152 , 0.75757927, 0.87674737, 0.94528437,\n",
      "       0.8844552 , 0.88667333, 0.8213956 , 0.8935128 , 0.87877375],\n",
      "      dtype=float32), array([0.89609975, 0.8907674 , 0.8317682 , 0.90842515, 0.91415983,\n",
      "       0.94131553, 0.88462543, 0.83679104, 0.91248083, 0.8736946 ],\n",
      "      dtype=float32), array([0.8919687 , 0.86697805, 0.80068225, 0.87593544, 0.88937396,\n",
      "       0.8691291 , 0.9347249 , 0.86060256, 0.9193522 , 0.8911198 ],\n",
      "      dtype=float32), array([0.90695286, 0.89633596, 0.7879577 , 0.8887504 , 0.907324  ,\n",
      "       0.8841411 , 0.92102975, 0.90200746, 0.92144203, 0.9241337 ],\n",
      "      dtype=float32), array([0.90546304, 0.89089566, 0.7660118 , 0.8725515 , 0.896135  ,\n",
      "       0.8718874 , 0.9116408 , 0.8548964 , 0.9391274 , 0.9135538 ],\n",
      "      dtype=float32), array([0.90849674, 0.8963612 , 0.781452  , 0.8807898 , 0.9017474 ,\n",
      "       0.8887086 , 0.92075   , 0.87986326, 0.927377  , 0.95887005],\n",
      "      dtype=float32)]\n",
      "1 8 0.8\n",
      "10 10 1.0\n"
     ]
    }
   ],
   "source": [
    "sent_similarity_topk(\n",
    "    model1=koenja_base, model2=koenja_base, \\\n",
    "    sents1=ko_koen_setntences, sents2=en_jabart_sentences[:10] , \\\n",
    "    d1=koenja_d, d2=koenja_d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mexican-venue",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "[array([0.62733096, 0.45849878, 0.37524152, 0.3588468 , 0.4197688 ,\n",
      "       0.4074183 , 0.3803748 , 0.29117808, 0.36261916, 0.28462642],\n",
      "      dtype=float32), array([0.40138695, 0.49521008, 0.3316309 , 0.34301957, 0.3484587 ,\n",
      "       0.39920777, 0.31767103, 0.29428077, 0.3458485 , 0.30130962],\n",
      "      dtype=float32), array([0.32192665, 0.35079235, 0.79198956, 0.5534144 , 0.53239304,\n",
      "       0.5197051 , 0.4423126 , 0.3429982 , 0.33676696, 0.273505  ],\n",
      "      dtype=float32), array([0.4476252 , 0.4222133 , 0.5505032 , 0.6753596 , 0.48915303,\n",
      "       0.51628774, 0.41857058, 0.34983388, 0.44404706, 0.3060086 ],\n",
      "      dtype=float32), array([0.41691366, 0.30735722, 0.37516597, 0.3803054 , 0.6869408 ,\n",
      "       0.41474575, 0.36312482, 0.3180371 , 0.3709499 , 0.30966386],\n",
      "      dtype=float32), array([0.34724092, 0.41902518, 0.5200223 , 0.5049714 , 0.50964004,\n",
      "       0.71659416, 0.38403064, 0.31761035, 0.39487284, 0.29666388],\n",
      "      dtype=float32), array([0.35167575, 0.35623065, 0.36319512, 0.3493652 , 0.33863032,\n",
      "       0.40168184, 0.63352484, 0.4458762 , 0.5048169 , 0.4505385 ],\n",
      "      dtype=float32), array([0.40820768, 0.38602886, 0.38950583, 0.37927413, 0.46024215,\n",
      "       0.4467797 , 0.58410436, 0.6634659 , 0.50567627, 0.5261234 ],\n",
      "      dtype=float32), array([0.3796527 , 0.35602883, 0.3252447 , 0.37933007, 0.34954894,\n",
      "       0.33572856, 0.49691594, 0.4133919 , 0.6122308 , 0.45520133],\n",
      "      dtype=float32), array([0.35887435, 0.31944004, 0.24722064, 0.29524422, 0.3245544 ,\n",
      "       0.3068699 , 0.53364563, 0.49923536, 0.46635538, 0.6779227 ],\n",
      "      dtype=float32)]\n",
      "1 10 1.0\n",
      "10 10 1.0\n"
     ]
    }
   ],
   "source": [
    "sent_similarity_topk(\n",
    "    model1=koenja_bart, model2=koenja_bart, \\\n",
    "    sents1=ko_koen_setntences, sents2=en_jabart_sentences[:10] , \\\n",
    "    d1=koenja_d, d2=koenja_d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "conscious-nutrition",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "[array([0.62593573, 0.44740704, 0.35251287, 0.41480747, 0.4190161 ,\n",
      "       0.42780688, 0.3480967 , 0.43163708, 0.412306  , 0.349921  ],\n",
      "      dtype=float32), array([0.41523075, 0.59781337, 0.31864694, 0.39931047, 0.39677545,\n",
      "       0.42269796, 0.40781367, 0.40736115, 0.41286013, 0.39752078],\n",
      "      dtype=float32), array([0.3015234 , 0.3140416 , 0.6738399 , 0.49981257, 0.37643817,\n",
      "       0.4894787 , 0.34595373, 0.3578904 , 0.2984012 , 0.3152542 ],\n",
      "      dtype=float32), array([0.4262741 , 0.44368464, 0.5446184 , 0.71414346, 0.45696956,\n",
      "       0.56215847, 0.43032426, 0.41910085, 0.4427546 , 0.3417266 ],\n",
      "      dtype=float32), array([0.39567402, 0.35154027, 0.3489403 , 0.41428366, 0.66027933,\n",
      "       0.3902354 , 0.3205379 , 0.3841722 , 0.36703518, 0.37736973],\n",
      "      dtype=float32), array([0.34219906, 0.4184179 , 0.5136139 , 0.5771184 , 0.43466803,\n",
      "       0.6834449 , 0.38274866, 0.4112187 , 0.4157455 , 0.40587112],\n",
      "      dtype=float32), array([0.38760516, 0.3872572 , 0.3614854 , 0.35133862, 0.39362928,\n",
      "       0.37764776, 0.62517095, 0.45685893, 0.4414009 , 0.41226453],\n",
      "      dtype=float32), array([0.40611333, 0.38045874, 0.35673934, 0.3714967 , 0.407433  ,\n",
      "       0.36830452, 0.4491607 , 0.6637007 , 0.44095975, 0.49198183],\n",
      "      dtype=float32), array([0.44515812, 0.39074948, 0.31091344, 0.37962133, 0.4208487 ,\n",
      "       0.34444648, 0.49475777, 0.48388663, 0.60917425, 0.4581187 ],\n",
      "      dtype=float32), array([0.33079755, 0.31262815, 0.2610994 , 0.27370444, 0.36135447,\n",
      "       0.32843968, 0.41798955, 0.4288158 , 0.41894954, 0.6407686 ],\n",
      "      dtype=float32)]\n",
      "1 10 1.0\n",
      "10 10 1.0\n"
     ]
    }
   ],
   "source": [
    "# koja (korean), jako (japanse)\n",
    "sent_similarity_topk(\n",
    "    model1=ft_koja, model2=ft_jako, \\\n",
    "    sents1=ko_koen_setntences, sents2=ja_enbart_sentences[:10] , \\\n",
    "    d1=koja_d, d2=koja_d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "durable-damage",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "1 1741 0.8705\n",
      "10 1943 0.9715\n"
     ]
    }
   ],
   "source": [
    "# koja (korean), jako (japanse)\n",
    "sent_similarity_topk(model1=ft_koja, model2=ft_jako, \\\n",
    "                     sents1=ko_jabart_sentences, sents2=ja_jabart_sentences , \\\n",
    "                     d1=koja_d, d2=koja_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "processed-madness",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "1 1050 0.525\n",
      "10 1645 0.8225\n"
     ]
    }
   ],
   "source": [
    "# koja (korean), jaBART (japanse)\n",
    "sent_similarity_topk(model1=ft_koja, model2=bart_jako, \\\n",
    "                     sents1=jako_sentences, sents2=koja_sentences, \\\n",
    "                     d1=koja_d, d2=koja_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "nonprofit-newark",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "1 1630 0.815\n",
      "10 1926 0.963\n"
     ]
    }
   ],
   "source": [
    "# koja (korean), jaen(japanse)\n",
    "sent_similarity_topk(model1=ft_koja, model2=ft_jaen, \\\n",
    "                     sents1=jako_sentences, sents2=koja_sentences, \\\n",
    "                     d1=koja_d, d2=enja_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-fleece",
   "metadata": {
    "tags": []
   },
   "source": [
    "### English and Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "yellow-notice",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "1 1478 0.739\n",
      "10 1777 0.8885\n"
     ]
    }
   ],
   "source": [
    "# enja (english), jaen (japanse)\n",
    "sent_similarity_topk(model1=ft_enja, model2=ft_jaen, \\\n",
    "                     sents1=jaen_sentences, sents2=enja_sentences, \\\n",
    "                     d1=enja_d, d2=enja_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "compound-legislature",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "1 1137 0.5685\n",
      "10 1670 0.835\n"
     ]
    }
   ],
   "source": [
    "# enja (english), jaBART (japanse)\n",
    "sent_similarity_topk(model1=ft_enja, model2=bart_enja, \\\n",
    "                     sents1=jaen_sentences, sents2=enja_sentences, \\\n",
    "                     d1=enja_d, d2=enja_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "atlantic-hammer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done cal vecs\n",
      "1 1334 0.667\n",
      "10 1738 0.869\n"
     ]
    }
   ],
   "source": [
    "# enja (english), jako (japanse)\n",
    "sent_similarity_topk(model1=ft_enja, model2=ft_jako, \\\n",
    "                     sents1=jaen_sentences, sents2=enja_sentences, \\\n",
    "                     d1=enja_d, d2=koja_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "progressive-classic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fusion_model(\n",
    "    model1_path=f\"{ft_jako_path}/{ft_jako_name}\", \n",
    "    model2_path=f\"{ft_jaen_path}/{ft_jaen_name}\", \n",
    "    save_path=\"test/fusion_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "plastic-butter",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "fusion_model(\n",
    "    model1_path=f\"{ft_jako_path}/{ft_jako_name}\", \n",
    "    model2_path=f\"{ft_jaen_path}/{ft_jaen_name}\", \n",
    "    save_path=\"fusion_jako_jaen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "equivalent-terror",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.layers.5.self_attn.k_proj.weight\n",
      "decoder.layers.5.self_attn.k_proj.bias\n",
      "decoder.layers.5.self_attn.v_proj.weight\n",
      "decoder.layers.5.self_attn.v_proj.bias\n",
      "decoder.layers.5.self_attn.q_proj.weight\n",
      "decoder.layers.5.self_attn.q_proj.bias\n",
      "decoder.layers.5.self_attn.out_proj.weight\n",
      "decoder.layers.5.self_attn.out_proj.bias\n",
      "decoder.layers.5.self_attn_layer_norm.weight\n",
      "decoder.layers.5.self_attn_layer_norm.bias\n",
      "decoder.layers.5.encoder_attn.k_proj.weight\n",
      "decoder.layers.5.encoder_attn.k_proj.bias\n",
      "decoder.layers.5.encoder_attn.v_proj.weight\n",
      "decoder.layers.5.encoder_attn.v_proj.bias\n",
      "decoder.layers.5.encoder_attn.q_proj.weight\n",
      "decoder.layers.5.encoder_attn.q_proj.bias\n",
      "decoder.layers.5.encoder_attn.out_proj.weight\n",
      "decoder.layers.5.encoder_attn.out_proj.bias\n",
      "decoder.layers.5.encoder_attn_layer_norm.weight\n",
      "decoder.layers.5.encoder_attn_layer_norm.bias\n",
      "decoder.layers.5.fc1.weight\n",
      "decoder.layers.5.fc1.bias\n",
      "decoder.layers.5.fc2.weight\n",
      "decoder.layers.5.fc2.bias\n",
      "decoder.layers.5.final_layer_norm.weight\n",
      "decoder.layers.5.final_layer_norm.bias\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "fusion_model(\n",
    "    model1_path=f\"{ft_jako_path}/{ft_jako_name}\", \n",
    "    model2_path=f\"{ft_jaen_path}/{ft_jaen_name}\", \n",
    "    save_path=\"fusion_jako_jaen_dec\",\n",
    "    dec_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "speaking-latitude",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bart_ende_path = \"../../enBART/bart.base/trim\"\n",
    "bart_ende_name = \"new_model.pt\"\n",
    "bart_ende = load_bart(path=bart_ende_path, model_name=bart_ende_name)\n",
    "bart_ende = bart_ende.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "automatic-responsibility",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ft_deen_path = \"../../enBART/ft_model\"\n",
    "ft_deen_name = \"checkpoint_best.pt\"\n",
    "ft_deen = load_bart(path=ft_deen_path, model_name=ft_deen_name)\n",
    "ft_deen = ft_deen.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dominican-russia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ft_jaen_path = \"../ja-en/enbart/checkpoints\"\n",
    "ft_jaen_name = \"checkpoint_best.pt\"\n",
    "ft_jaen_enbart = load_bart(path=ft_jaen_path, model_name=ft_jaen_name)\n",
    "ft_jaen_enbart = ft_jaen_enbart.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "foster-devon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ende_d = load_dict(ft_deen_path + \"/\" + \"dict.en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-spain",
   "metadata": {},
   "source": [
    "## CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "wireless-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5122 14305 286 2489 318 262 717 286 428 39210 13\n",
      "\n",
      "tensor([ 1429,  3678,     9,   224,    23,     6,   246,     9,    35, 13064,\n",
      "            5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> de droits 11 c 6 395 le premier de ce millnaire 13 </s>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = enbart_fren_sentences_en[0]\n",
    "token_ids = sent_to_ids(d=enfr_enbart_d, sent=sent)\n",
    "print(sent)\n",
    "print(token_ids)\n",
    "generated = generate(token_ids.unsqueeze(0).to(DEVICE), enbart_ft_enfr)\n",
    "\" \".join(ids_to_tokens(enfr_enbart_d, generated[-1][0]['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stainless-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          \n",
      "\n",
      "tensor([ 1653,     4,  2146,   925,  2663,  4095,    10,   594,   118,     8,\n",
      "            4,   117, 12279,   332,  2146,   925,   222,    10,   218,    15,\n",
      "          341,    17,    93,  1448, 12837,   529,     7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> 8875 341 286 262 29358 15879 318 11282 11 2158 11 1201 340 318 407 3306 284 4886 262 11410 29358 4571 15370 13 </s>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = enbart_jaen_sentences_ja[0]\n",
    "token_ids = sent_to_ids(d=enja_enbart_d, sent=sent)\n",
    "print(sent)\n",
    "print(token_ids)\n",
    "generated = generate(token_ids.unsqueeze(0).to(DEVICE), enbart_ft_jaen)\n",
    "\" \".join(ids_to_tokens(enja_enbart_d, generated[-1][0]['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "centered-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "Encoder CKA\n",
      "Layer 0 0.32811857454424265\n",
      "Layer 1 0.5546532730269308\n",
      "Layer 2 0.5995680525914352\n",
      "Layer 3 0.6204308990722559\n",
      "Layer 4 0.6279079745304093\n",
      "Layer 5 0.5875804892141104\n",
      "Layer 6 0.6083146674879423\n",
      "Layer 7 0.6083146674879423\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.09744597521954425\n",
      "Layer 1 0.15249698280794938\n",
      "Layer 2 0.210876683315914\n",
      "Layer 3 0.23154576349387865\n",
      "Layer 4 0.22852527316730292\n",
      "Layer 5 0.2448341766431118\n",
      "Layer 6 0.3007433409915509\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.0939722852607291\n",
      "Layer 1 0.15361276708531657\n",
      "Layer 2 0.2028390013781639\n",
      "Layer 3 0.22414851803754945\n",
      "Layer 4 0.25860800875306866\n",
      "Layer 5 0.23398130362536687\n"
     ]
    }
   ],
   "source": [
    "# enBART, jaen\n",
    "encdec_cka_sim(\n",
    "    pre=enbart_enja, ft=enbart_ft_jaen, \\\n",
    "    pre_d=enja_enbart_d, ft_d=enja_enbart_d, \\\n",
    "    pre_sentences=enbart_jaen_sentences_en[:1000], ft_sentences=enbart_jaen_sentences_ja[:1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "supposed-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "Encoder CKA\n",
      "Layer 0 0.9710718630659055\n",
      "Layer 1 0.8120210146657719\n",
      "Layer 2 0.8006389337341203\n",
      "Layer 3 0.7654746487379535\n",
      "Layer 4 0.7380350953301268\n",
      "Layer 5 0.7333817872153834\n",
      "Layer 6 0.7386213187787545\n",
      "Layer 7 0.7386213187787545\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.06769563267976879\n",
      "Layer 1 0.12355517600508875\n",
      "Layer 2 0.1915150468153713\n",
      "Layer 3 0.23190734801412766\n",
      "Layer 4 0.22419532676016332\n",
      "Layer 5 0.24174943072293711\n",
      "Layer 6 0.4389424708551309\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.08121777781972517\n",
      "Layer 1 0.14379190379947235\n",
      "Layer 2 0.2034657652326823\n",
      "Layer 3 0.22121958176162368\n",
      "Layer 4 0.2543349435979615\n",
      "Layer 5 0.23425001394704456\n"
     ]
    }
   ],
   "source": [
    "# enBART, enja\n",
    "encdec_cka_sim(\n",
    "    pre=enbart_enja, ft=enbart_ft_enja, \\\n",
    "    pre_d=enja_enbart_d, ft_d=enja_enbart_d, \\\n",
    "    pre_sentences=enbart_jaen_sentences_en[:1000], ft_sentences=enbart_jaen_sentences_en[:1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acknowledged-kennedy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encdec_cka_sim_test(\n",
    "    pre, ft, \n",
    "    pre_d, ft_d, \n",
    "    pre_sentences, ft_sentences\n",
    "):\n",
    "    pre_enc_d = defaultdict(list)\n",
    "    pre_dec_d = defaultdict(list)\n",
    "    pre_dec_self_attn_d = defaultdict(list)\n",
    "    \n",
    "    ft_dec_d = defaultdict(list)\n",
    "    ft_enc_d = defaultdict(list)\n",
    "    ft_dec_self_attn_d = defaultdict(list)\n",
    "    \n",
    "    for n, (pre_sent, ft_sent) in enumerate(zip(pre_sentences, ft_sentences)):\n",
    "        if (n+1) % 200 == 0:\n",
    "            print(f\"done {n+1}\")\n",
    "        pre_sent = pre_sent.strip()\n",
    "        ft_sent = ft_sent.strip()\n",
    "#         print(pre_sent)\n",
    "        pre_enc_vecs, pre_dec_vecs = encdec_cal_mean(model=pre, d=pre_d, sent=pre_sent)\n",
    "#         print(ft_sent)\n",
    "        ft_enc_vecs, ft_dec_vecs = encdec_cal_mean(model=ft, d=ft_d, sent=ft_sent)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_enc_vecs, ft_enc_vecs)):\n",
    "            pre_enc_d[n].append(pre_vec)\n",
    "            ft_enc_d[n].append(ft_vec)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_dec_vecs[\"inner_mean_layers\"], \\\n",
    "                                                  ft_dec_vecs[\"inner_mean_layers\"])):\n",
    "            pre_dec_d[n].append(pre_vec)\n",
    "            ft_dec_d[n].append(ft_vec)\n",
    "        \n",
    "        for n, (pre_vec, ft_vec) in enumerate(zip(pre_dec_vecs[\"self_attn_mean_layers\"], \\\n",
    "                                                  ft_dec_vecs[\"self_attn_mean_layers\"])):\n",
    "            pre_dec_self_attn_d[n].append(pre_vec)\n",
    "            ft_dec_self_attn_d[n].append(ft_vec)\n",
    "    \n",
    "    return pre_enc_d, ft_enc_d, pre_dec_d, ft_dec_d, pre_dec_self_attn_d, ft_dec_self_attn_d \n",
    "    \n",
    "#     print(\"Encoder CKA\")\n",
    "#     for key in pre_enc_d.keys():\n",
    "#         pre_matrix = np.array(pre_enc_d[key])\n",
    "#         ft_matrix = np.array(ft_enc_d[key])\n",
    "# #         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "#         print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "    \n",
    "#     print(\"\\nDecoder CKA\")\n",
    "#     for key in pre_dec_d.keys():\n",
    "#         pre_matrix = np.array(pre_dec_d[key])\n",
    "#         ft_matrix = np.array(ft_dec_d[key])\n",
    "# #         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "#         print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "    \n",
    "#     print(\"\\nDecoder Self Attention CKA\")\n",
    "#     for key in pre_dec_self_attn_d.keys():\n",
    "#         pre_matrix = np.array(pre_dec_self_attn_d[key])\n",
    "#         ft_matrix = np.array(ft_dec_self_attn_d[key])\n",
    "# #         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "#         print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rotary-fiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "encoder\n",
      "Layer 0 0.9729578273360813\n",
      "Layer 1 0.863921782443766\n",
      "Layer 2 0.83642350711511\n",
      "Layer 3 0.8197397063118146\n",
      "Layer 4 0.7979322622420596\n",
      "Layer 5 0.7767335145898732\n",
      "Layer 6 0.7917862164124667\n",
      "Layer 7 0.7917862164124667\n",
      "decoder\n",
      "Layer 0 0.13771677898439774\n",
      "Layer 1 0.2756676511382894\n",
      "Layer 2 0.479574223248925\n",
      "Layer 3 0.5311533728022519\n",
      "Layer 4 0.4342821369419174\n",
      "Layer 5 0.37446123791379327\n",
      "Layer 6 0.4570160011679051\n",
      "self attn\n",
      "Layer 0 0.1658540749843371\n",
      "Layer 1 0.3927683910431462\n",
      "Layer 2 0.5151794108942672\n",
      "Layer 3 0.5453780886867194\n",
      "Layer 4 0.5104035620170355\n",
      "Layer 5 0.39448659546226084\n"
     ]
    }
   ],
   "source": [
    "# enBART, enfr\n",
    "pre_enc_d, ft_enc_d, pre_dec_d, ft_dec_d, pre_dec_self_attn_d, ft_dec_self_attn_d  = encdec_cka_sim_test(\n",
    "    pre=enbart_enfr, ft=enbart_ft_enfr, \\\n",
    "    pre_d=enfr_enbart_d, ft_d=enfr_enbart_d, \\\n",
    "    pre_sentences=enbart_fren_sentences_en[:1000], ft_sentences=enbart_fren_sentences_en[:1000]\n",
    ")\n",
    "\n",
    "import copy\n",
    "pre_enc_d_ = copy.deepcopy(pre_enc_d)\n",
    "ft_enc_d_ = copy.deepcopy(ft_enc_d)\n",
    "for key in pre_enc_d_.keys():\n",
    "    pre_enc_d_[key].pop(301)\n",
    "    ft_enc_d_[key].pop(301)\n",
    "    \n",
    "# encoder\n",
    "print('encoder')\n",
    "for key in pre_enc_d_.keys():\n",
    "    pre_matrix = np.array(pre_enc_d_[key])\n",
    "    ft_matrix = np.array(ft_enc_d_[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "\n",
    "    \n",
    "# decoder\n",
    "print('decoder')\n",
    "for key in pre_dec_d.keys():\n",
    "    pre_matrix = np.array(pre_dec_d[key])\n",
    "    ft_matrix = np.array(ft_dec_d[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "\n",
    "# decoder\n",
    "print('self attn')\n",
    "for key in pre_dec_self_attn_d.keys():\n",
    "    pre_matrix = np.array(pre_dec_self_attn_d[key])\n",
    "    ft_matrix = np.array(ft_dec_self_attn_d[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "greenhouse-experiment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n"
     ]
    }
   ],
   "source": [
    "# enBART, fren\n",
    "pre_enc_d, ft_enc_d, pre_dec_d, ft_dec_d, pre_dec_self_attn_d, ft_dec_self_attn_d  = encdec_cka_sim_test(\n",
    "    pre=enbart_enfr, ft=enbart_ft_fren, \\\n",
    "    pre_d=enfr_enbart_d, ft_d=enfr_enbart_d, \\\n",
    "    pre_sentences=enbart_fren_sentences_en[:1000], ft_sentences=enbart_fren_sentences_fr[:1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "responsible-liver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder\n",
      "Layer 0 0.1752605944275357\n",
      "Layer 1 0.6512307162908527\n",
      "Layer 2 0.6710861384363259\n",
      "Layer 3 0.6888394556049736\n",
      "Layer 4 0.6883013824515215\n",
      "Layer 5 0.6883653756778685\n",
      "Layer 6 0.702137791005405\n",
      "Layer 7 0.702137791005405\n",
      "decoder\n",
      "Layer 0 0.1904847206331985\n",
      "Layer 1 0.3102052891602589\n",
      "Layer 2 0.5059126944672591\n",
      "Layer 3 0.5266502616683636\n",
      "Layer 4 0.4802726042683419\n",
      "Layer 5 0.38671790971708636\n",
      "Layer 6 0.47487189395441165\n",
      "self attn\n",
      "Layer 0 0.19369339189243265\n",
      "Layer 1 0.41890349497784435\n",
      "Layer 2 0.5157724904991667\n",
      "Layer 3 0.5434187574018567\n",
      "Layer 4 0.501903158446825\n",
      "Layer 5 0.4457863845303657\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "pre_enc_d_ = copy.deepcopy(pre_enc_d)\n",
    "ft_enc_d_ = copy.deepcopy(ft_enc_d)\n",
    "for key in pre_enc_d_.keys():\n",
    "    pre_enc_d_[key].pop(301)\n",
    "    ft_enc_d_[key].pop(301)\n",
    "    \n",
    "# encoder\n",
    "print('encoder')\n",
    "for key in pre_enc_d_.keys():\n",
    "    pre_matrix = np.array(pre_enc_d_[key])\n",
    "    ft_matrix = np.array(ft_enc_d_[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "\n",
    "    \n",
    "# decoder\n",
    "print('decoder')\n",
    "for key in pre_dec_d.keys():\n",
    "    pre_matrix = np.array(pre_dec_d[key])\n",
    "    ft_matrix = np.array(ft_dec_d[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))\n",
    "\n",
    "# decoder\n",
    "print('self attn')\n",
    "for key in pre_dec_self_attn_d.keys():\n",
    "    pre_matrix = np.array(pre_dec_self_attn_d[key])\n",
    "    ft_matrix = np.array(ft_dec_self_attn_d[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "empirical-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0.3686674118939949\n",
      "Layer 1 0.764543460352847\n",
      "Layer 2 0.7732998699589144\n",
      "Layer 3 0.7783773902712067\n",
      "Layer 4 0.7768667884210361\n",
      "Layer 5 0.7649257051404549\n",
      "Layer 6 0.75241364353149\n",
      "Layer 7 0.75241364353149\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "for key in list(pre_enc_d_.keys()):\n",
    "    pre_matrix = np.array(pre_enc_d_[key][:100])\n",
    "    ft_matrix = np.array(ft_enc_d_[key][:100])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "identified-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0.1751167957811808\n",
      "Layer 1 0.6485063065422028\n",
      "Layer 2 0.6682326277343577\n",
      "Layer 3 0.6869086617683943\n",
      "Layer 4 0.6869959584973313\n",
      "Layer 5 0.6876304094033615\n",
      "Layer 6 0.701408444348908\n",
      "Layer 7 0.701408444348908\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "for key in list(pre_enc_d_.keys()):\n",
    "    pre_matrix = np.array(pre_enc_d_[key][:1000])\n",
    "    ft_matrix = np.array(ft_enc_d_[key][:1000])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "established-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0.1961328198444898\n",
      "Layer 1 0.6416696873840658\n",
      "Layer 2 0.6561150719027569\n",
      "Layer 3 0.6778685181706523\n",
      "Layer 4 0.6813434228322837\n",
      "Layer 5 0.6833915482410283\n",
      "Layer 6 0.6987504653861911\n",
      "Layer 7 0.6987504653861911\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "for key in pre_enc_d_.keys():\n",
    "    pre_matrix = np.array(pre_enc_d_[key])\n",
    "    ft_matrix = np.array(ft_enc_d_[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "better-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0.17552321581938815\n",
      "Layer 1 0.28511377663220694\n",
      "Layer 2 0.47311819037920333\n",
      "Layer 3 0.5033558407798207\n",
      "Layer 4 0.4611915764592244\n",
      "Layer 5 0.3678856021885661\n",
      "Layer 6 0.465555820709266\n"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "for key in pre_dec_d.keys():\n",
    "    pre_matrix = np.array(pre_dec_d[key])\n",
    "    ft_matrix = np.array(ft_dec_d[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "generic-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 0.1837217050701935\n",
      "Layer 1 0.3856648766373253\n",
      "Layer 2 0.4846951074641052\n",
      "Layer 3 0.5186218163180184\n",
      "Layer 4 0.48464657473181366\n",
      "Layer 5 0.4293935656848947\n"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "for key in pre_dec_self_attn_d.keys():\n",
    "    pre_matrix = np.array(pre_dec_self_attn_d[key])\n",
    "    ft_matrix = np.array(ft_dec_self_attn_d[key])\n",
    "#     print(pre_matrix.shape, ft_matrix.shape)\n",
    "#         print(\"v1\", key, cal_cka_sim(jp_bart_matrix, ft_matrix))\n",
    "    print(\"Layer\", key, linear_CKA(pre_matrix, ft_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "approximate-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9780712565411755\n",
      "Layer 1 0.8937279811922552\n",
      "Layer 2 0.8658691974817965\n",
      "Layer 3 0.8453532725587504\n",
      "Layer 4 0.8418310600469964\n",
      "Layer 5 0.8242200640133998\n",
      "Layer 6 0.8231984675617963\n",
      "Layer 7 0.8231984675617963\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.25038420598361627\n",
      "Layer 1 0.343640572874969\n",
      "Layer 2 0.5370430129241208\n",
      "Layer 3 0.5962060892498431\n",
      "Layer 4 0.5279244853339019\n",
      "Layer 5 0.45512418680039474\n",
      "Layer 6 0.6828338541700857\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.2546367824372348\n",
      "Layer 1 0.4726495006136006\n",
      "Layer 2 0.5777921159012347\n",
      "Layer 3 0.6183637663639528\n",
      "Layer 4 0.6065769455032194\n",
      "Layer 5 0.5498576622801526\n"
     ]
    }
   ],
   "source": [
    "# enBART, enfr\n",
    "encdec_cka_sim(\n",
    "    pre=enbart_enfr, ft=enbart_ft_enfr, \\\n",
    "    pre_d=enfr_enbart_d, ft_d=enfr_enbart_d, \\\n",
    "    pre_sentences=enbart_fren_sentences_en[:100], ft_sentences=enbart_fren_sentences_en[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "painted-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "done 1200\n",
      "done 1400\n",
      "done 1600\n",
      "done 1800\n",
      "done 2000\n",
      "Encoder CKA\n",
      "Layer 0 0.9733945818732439\n",
      "Layer 1 0.8687681675939442\n",
      "Layer 2 0.8434158785275693\n",
      "Layer 3 0.9370607349495887\n",
      "Layer 4 0.9614628335582258\n",
      "Layer 5 0.9700674107880242\n",
      "Layer 6 0.9635520520292242\n",
      "Layer 7 0.7176681915201348\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.16763322071215492\n",
      "Layer 1 0.5132706661887682\n",
      "Layer 2 0.5282737227516087\n",
      "Layer 3 0.5554090535647359\n",
      "Layer 4 0.519369563986752\n",
      "Layer 5 0.43157912157737083\n",
      "Layer 6 0.02994465836785132\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.29671080036439834\n",
      "Layer 1 0.5727299236081045\n",
      "Layer 2 0.5479109091949836\n",
      "Layer 3 0.49175863056010855\n",
      "Layer 4 0.415207337167861\n",
      "Layer 5 0.222258803687259\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jako\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_jako, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=jako_sentences_ja, ft_sentences=jako_sentences_ja\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "freelance-intellectual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9759973298327697\n",
      "Layer 1 0.9256172266120095\n",
      "Layer 2 0.8741730657158969\n",
      "Layer 3 0.9465301895243656\n",
      "Layer 4 0.969123673766228\n",
      "Layer 5 0.9790643359818927\n",
      "Layer 6 0.9741754062303096\n",
      "Layer 7 0.8699713363496407\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.26566675837396675\n",
      "Layer 1 0.6244883834420452\n",
      "Layer 2 0.6167888731680147\n",
      "Layer 3 0.6883192185988544\n",
      "Layer 4 0.6888598779958519\n",
      "Layer 5 0.6522734899382354\n",
      "Layer 6 0.07072162457780592\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.37322840753426545\n",
      "Layer 1 0.6726726791306216\n",
      "Layer 2 0.659283002403375\n",
      "Layer 3 0.6400832383024861\n",
      "Layer 4 0.6084594033549899\n",
      "Layer 5 0.3837667729528462\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jako\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_jako, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=jako_sentences_ja[:100], ft_sentences=jako_sentences_ja[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "outer-mirror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9729115896163111\n",
      "Layer 1 0.9135987907704255\n",
      "Layer 2 0.821561152461635\n",
      "Layer 3 0.8680825866974448\n",
      "Layer 4 0.9170677291728615\n",
      "Layer 5 0.9297949430319115\n",
      "Layer 6 0.9092694707275283\n",
      "Layer 7 0.8168259950394288\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.4915655752079639\n",
      "Layer 1 0.6809627218543579\n",
      "Layer 2 0.6104667347317372\n",
      "Layer 3 0.6451872520219702\n",
      "Layer 4 0.5917880699782185\n",
      "Layer 5 0.5479433708283096\n",
      "Layer 6 0.09042188181138364\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.5492166624244063\n",
      "Layer 1 0.7044201948904427\n",
      "Layer 2 0.6110072979431107\n",
      "Layer 3 0.5474638884843045\n",
      "Layer 4 0.49048689881554514\n",
      "Layer 5 0.3571971711707886\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jako\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_jako, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=jaen_sentences_ja[:100], ft_sentences=jaen_sentences_ja[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "recreational-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.970523551005553\n",
      "Layer 1 0.9151731549531593\n",
      "Layer 2 0.8277985826173108\n",
      "Layer 3 0.9280565330505587\n",
      "Layer 4 0.960762716719473\n",
      "Layer 5 0.9697891275931673\n",
      "Layer 6 0.9642266387392431\n",
      "Layer 7 0.8444771900088857\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.4263150415348768\n",
      "Layer 1 0.571452449384793\n",
      "Layer 2 0.6663581064078352\n",
      "Layer 3 0.7303663734183071\n",
      "Layer 4 0.6982537767593614\n",
      "Layer 5 0.6197065847507601\n",
      "Layer 6 0.08705909603272607\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.48895318702558804\n",
      "Layer 1 0.6127027821668646\n",
      "Layer 2 0.704060883088779\n",
      "Layer 3 0.6916327937374782\n",
      "Layer 4 0.6116822922622422\n",
      "Layer 5 0.4720994915017967\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jaen\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_jaen, \\\n",
    "    pre_d=koja_d, ft_d=enja_d, \\\n",
    "    pre_sentences=jaen_sentences_ja[:100], ft_sentences=jaen_sentences_ja[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "private-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "done 1200\n",
      "done 1400\n",
      "done 1600\n",
      "done 1800\n",
      "done 2000\n",
      "Encoder CKA\n",
      "Layer 0 0.9698217565326317\n",
      "Layer 1 0.8682271338603668\n",
      "Layer 2 0.830867335717296\n",
      "Layer 3 0.9593129674841185\n",
      "Layer 4 0.9793769867646098\n",
      "Layer 5 0.9828426449643142\n",
      "Layer 6 0.9808061057361167\n",
      "Layer 7 0.7834330659491489\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.16921201369717648\n",
      "Layer 1 0.437809268078041\n",
      "Layer 2 0.5526726631509171\n",
      "Layer 3 0.6140158325860415\n",
      "Layer 4 0.6323702549712223\n",
      "Layer 5 0.525754290394626\n",
      "Layer 6 0.049906383588926094\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.2743104183527964\n",
      "Layer 1 0.48853681863761567\n",
      "Layer 2 0.5983830092594575\n",
      "Layer 3 0.6143103270959407\n",
      "Layer 4 0.5513991431027618\n",
      "Layer 5 0.3425840837702103\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jaen\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_jaen, \\\n",
    "    pre_d=koja_d, ft_d=enja_d, \\\n",
    "    pre_sentences=jako_sentences_ja, ft_sentences=jako_sentences_ja\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "social-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.970523551005553\n",
      "Layer 1 0.9151731549531593\n",
      "Layer 2 0.8277985826173108\n",
      "Layer 3 0.9280565330505587\n",
      "Layer 4 0.960762716719473\n",
      "Layer 5 0.9697891275931673\n",
      "Layer 6 0.9642266387392431\n",
      "Layer 7 0.8444771900088857\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.4263150415348768\n",
      "Layer 1 0.571452449384793\n",
      "Layer 2 0.6663581064078352\n",
      "Layer 3 0.7303663734183071\n",
      "Layer 4 0.6982537767593614\n",
      "Layer 5 0.6197065847507601\n",
      "Layer 6 0.08705909603272607\n"
     ]
    }
   ],
   "source": [
    "# jaen, jaBART\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_jaen, \\\n",
    "    pre_d=koja_d, ft_d=enja_d, \\\n",
    "    pre_sentences=ja_jaenbart_sentences[:100], ft_sentences=ja_jaenbart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "minimal-skirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9674408329461288\n",
      "Layer 1 0.9343761404528619\n",
      "Layer 2 0.75086902735202\n",
      "Layer 3 0.7450048805260591\n",
      "Layer 4 0.8005402722944482\n",
      "Layer 5 0.832185747041194\n",
      "Layer 6 0.8449792487178245\n",
      "Layer 7 0.8687638817449904\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.3829109775723496\n",
      "Layer 1 0.6179080335070457\n",
      "Layer 2 0.567610266163832\n",
      "Layer 3 0.6538575803829098\n",
      "Layer 4 0.6969924748737887\n",
      "Layer 5 0.6247609008165051\n",
      "Layer 6 0.051791402605285794\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jako_fa\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako_fa, ft=ft_jako_fa, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ja_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "usual-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.5093697481332495\n",
      "Layer 1 0.8607331305958683\n",
      "Layer 2 0.7021390384058981\n",
      "Layer 3 0.5819528201946342\n",
      "Layer 4 0.4928926286905733\n",
      "Layer 5 0.49713961149770364\n",
      "Layer 6 0.5818514551276623\n",
      "Layer 7 0.8340159722100928\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.5510239926259972\n",
      "Layer 1 0.7549550905931844\n",
      "Layer 2 0.6667061462110943\n",
      "Layer 3 0.7398014686717572\n",
      "Layer 4 0.735674058974253\n",
      "Layer 5 0.6852968467697668\n",
      "Layer 6 0.07426608661909906\n"
     ]
    }
   ],
   "source": [
    "# jaBART, jako_fa\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ko_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "british-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.486273220571243\n",
      "Layer 1 0.8595038398785774\n",
      "Layer 2 0.7033624169068726\n",
      "Layer 3 0.5827525385954249\n",
      "Layer 4 0.4929243158033353\n",
      "Layer 5 0.49678832290368524\n",
      "Layer 6 0.5807955722898139\n",
      "Layer 7 0.8324206411740162\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.4713994422494003\n",
      "Layer 1 0.6870161440890267\n",
      "Layer 2 0.6200601975537867\n",
      "Layer 3 0.7012665521444087\n",
      "Layer 4 0.6967926584543497\n",
      "Layer 5 0.6513957015346789\n",
      "Layer 6 0.07202165970481228\n"
     ]
    }
   ],
   "source": [
    "# jaBART, koja_fa\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako_fa, ft=ft_koja_fa, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ko_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "concrete-blind",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.2133956818154692\n",
      "Layer 1 0.7509003912833928\n",
      "Layer 2 0.6922122203834341\n",
      "Layer 3 0.6069842822146548\n",
      "Layer 4 0.5733573434807498\n",
      "Layer 5 0.5827822053694163\n",
      "Layer 6 0.6114356266047001\n",
      "Layer 7 0.778814152549551\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.6823823424997126\n",
      "Layer 1 0.6796465853609647\n",
      "Layer 2 0.4943844554526894\n",
      "Layer 3 0.6165263221615128\n",
      "Layer 4 0.7040800035040596\n",
      "Layer 5 0.6710977798490118\n",
      "Layer 6 0.13769230941600222\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.6551121034156565\n",
      "Layer 1 0.7128061060160942\n",
      "Layer 2 0.5553544668569662\n",
      "Layer 3 0.678281670753409\n",
      "Layer 4 0.6748196715070189\n",
      "Layer 5 0.5423299911263483\n"
     ]
    }
   ],
   "source": [
    "# enja, jaBART\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_enja, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sentences=jaen_sentences_ja[:100], ft_sentences=jaen_sentences_en[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "reverse-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.2133956818154692\n",
      "Layer 1 0.7509003912833928\n",
      "Layer 2 0.6922122203834341\n",
      "Layer 3 0.6069842822146548\n",
      "Layer 4 0.5733573434807498\n",
      "Layer 5 0.5827822053694163\n",
      "Layer 6 0.6114356266047001\n",
      "Layer 7 0.778814152549551\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.6823823424997126\n",
      "Layer 1 0.6796465853609647\n",
      "Layer 2 0.4943844554526894\n",
      "Layer 3 0.6165263221615128\n",
      "Layer 4 0.7040800035040596\n",
      "Layer 5 0.6710977798490118\n",
      "Layer 6 0.13769230941600222\n"
     ]
    }
   ],
   "source": [
    "# enja, jaBART\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_enja, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=en_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "alleged-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.21473489816154617\n",
      "Layer 1 0.7498205167981\n",
      "Layer 2 0.691505666374513\n",
      "Layer 3 0.6068007578536602\n",
      "Layer 4 0.573210229228657\n",
      "Layer 5 0.5827459905508178\n",
      "Layer 6 0.6114135420006851\n",
      "Layer 7 0.7790729070819575\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.6824959605189248\n",
      "Layer 1 0.6797730053523041\n",
      "Layer 2 0.49437331951835245\n",
      "Layer 3 0.6168067794802606\n",
      "Layer 4 0.7047705761430938\n",
      "Layer 5 0.6709161777008913\n",
      "Layer 6 0.1376089437361425\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim_rm_special_tokens(\n",
    "    pre=jabart_jako, ft=ft_enja, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sentences=ja_jaenbart_sentences[:100], ft_sentences=en_jaenbart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "informative-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.44956301094439244\n",
      "Layer 1 0.7614944188015746\n",
      "Layer 2 0.6400164042235462\n",
      "Layer 3 0.5321559040794689\n",
      "Layer 4 0.5105673216760417\n",
      "Layer 5 0.5171827178674153\n",
      "Layer 6 0.553711656819959\n",
      "Layer 7 0.8278560807765531\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.638503693999009\n",
      "Layer 1 0.5501354377787423\n",
      "Layer 2 0.4896567254842808\n",
      "Layer 3 0.5422355298174486\n",
      "Layer 4 0.6054068802227315\n",
      "Layer 5 0.700545378921386\n",
      "Layer 6 0.6799855957357414\n"
     ]
    }
   ],
   "source": [
    "# enja, jaen\n",
    "encdec_cka_sim(pre=ft_jaen, ft=ft_enja, pre_d=enja_d, ft_d=enja_d, pre_sentences=enja_sentences[:100], ft_sentences=jaen_sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "spare-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.3942145081540587\n",
      "Layer 1 0.829636910217655\n",
      "Layer 2 0.8718990500765171\n",
      "Layer 3 0.9163439671916835\n",
      "Layer 4 0.9247556100999105\n",
      "Layer 5 0.9253361288367854\n",
      "Layer 6 0.9351865740131241\n",
      "Layer 7 0.8551808660096432\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.5268097265235973\n",
      "Layer 1 0.7539800268719123\n",
      "Layer 2 0.7297802524794936\n",
      "Layer 3 0.7843447525690713\n",
      "Layer 4 0.7566099680784231\n",
      "Layer 5 0.7428515601808281\n",
      "Layer 6 0.08962926898552133\n",
      "\n",
      "Decoder Self Attention CKA\n",
      "Layer 0 0.5117977862552471\n",
      "Layer 1 0.7677623124738384\n",
      "Layer 2 0.7748265491152246\n",
      "Layer 3 0.7346492567630117\n",
      "Layer 4 0.6835754858209302\n",
      "Layer 5 0.5693995840357118\n"
     ]
    }
   ],
   "source": [
    "# koja, jaBART\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=jako_sentences_ja[:100], ft_sentences=jako_sentences_ko[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "million-australian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.39813758667059446\n",
      "Layer 1 0.8372665742577943\n",
      "Layer 2 0.884557799175998\n",
      "Layer 3 0.9167199884185456\n",
      "Layer 4 0.9193502656476504\n",
      "Layer 5 0.9218732232526243\n",
      "Layer 6 0.9340356606040892\n",
      "Layer 7 0.8588061707660819\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.5050652878275668\n",
      "Layer 1 0.7533884376494239\n",
      "Layer 2 0.7310251120235626\n",
      "Layer 3 0.7696943537806099\n",
      "Layer 4 0.7340218602447084\n",
      "Layer 5 0.7219580688844796\n",
      "Layer 6 0.06652651826563986\n"
     ]
    }
   ],
   "source": [
    "# koja, jaBART\n",
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ko_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "synthetic-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.39748979105451354\n",
      "Layer 1 0.8296825798725999\n",
      "Layer 2 0.8719219192291665\n",
      "Layer 3 0.9163728779845367\n",
      "Layer 4 0.9247754575194317\n",
      "Layer 5 0.9253441818207621\n",
      "Layer 6 0.9352153326508204\n",
      "Layer 7 0.8556609981027861\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.5292802317066537\n",
      "Layer 1 0.7537436719615525\n",
      "Layer 2 0.7294155346030693\n",
      "Layer 3 0.7841146349196392\n",
      "Layer 4 0.7570655568098495\n",
      "Layer 5 0.7434283779149016\n",
      "Layer 6 0.0895921941697945\n"
     ]
    }
   ],
   "source": [
    "# koja, jaBART\n",
    "encdec_cka_sim_rm_special_tokens(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ko_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "requested-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.39813758667059446\n",
      "Layer 1 0.8372665742577943\n",
      "Layer 2 0.884557799175998\n",
      "Layer 3 0.9167199884185456\n",
      "Layer 4 0.9193502656476504\n",
      "Layer 5 0.9218732232526243\n",
      "Layer 6 0.9340356606040892\n",
      "Layer 7 0.8588061707660819\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.5050652878275668\n",
      "Layer 1 0.7533884376494239\n",
      "Layer 2 0.7310251120235626\n",
      "Layer 3 0.7696943537806099\n",
      "Layer 4 0.7340218602447084\n",
      "Layer 5 0.7219580688844796\n",
      "Layer 6 0.06652651826563986\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ko_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "royal-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.39813758667059446\n",
      "Layer 1 0.8372665742577943\n",
      "Layer 2 0.884557799175998\n",
      "Layer 3 0.9167199884185456\n",
      "Layer 4 0.9193502656476504\n",
      "Layer 5 0.9218732232526243\n",
      "Layer 6 0.9340356606040892\n",
      "Layer 7 0.8588061707660819\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.18737031889213474\n",
      "Layer 1 0.27824082522955756\n",
      "Layer 2 0.2912734439356014\n",
      "Layer 3 0.3114805956735732\n",
      "Layer 4 0.3444135556508579\n",
      "Layer 5 0.293009997229996\n",
      "Layer 6 0.0256687581392078\n"
     ]
    }
   ],
   "source": [
    "# koja, jaBART use last token's hidden states\n",
    "encdec_cka_sim_v2(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:100], ft_sentences=ko_jabart_sentences[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "graduate-johnson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.5447266789603019\n",
      "Layer 1 0.9408569768496966\n",
      "Layer 2 0.89988828928416\n",
      "Layer 3 0.882193235810978\n",
      "Layer 4 0.8761381932163314\n",
      "Layer 5 0.8806489404925734\n",
      "Layer 6 0.9109798977908943\n",
      "Layer 7 0.9425632468949836\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.6717251359159946\n",
      "Layer 1 0.8700682889912937\n",
      "Layer 2 0.837079933312262\n",
      "Layer 3 0.8903882350217927\n",
      "Layer 4 0.8628185237805014\n",
      "Layer 5 0.8448014053877598\n",
      "Layer 6 0.4144622661368667\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[:20], ft_sentences=ko_jabart_sentences[:20]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "corporate-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.3692928894893377\n",
      "Layer 1 0.816572963563203\n",
      "Layer 2 0.8919192527464466\n",
      "Layer 3 0.9184192672084958\n",
      "Layer 4 0.9162769446453053\n",
      "Layer 5 0.9181849883188986\n",
      "Layer 6 0.9320291458613279\n",
      "Layer 7 0.8327011045213755\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.13916792912710638\n",
      "Layer 1 0.2121558555510714\n",
      "Layer 2 0.2224732547177916\n",
      "Layer 3 0.24894895052957788\n",
      "Layer 4 0.2772183862086337\n",
      "Layer 5 0.21853501853480337\n",
      "Layer 6 0.03117152116482811\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim_v2(\n",
    "    pre=jabart_jako, ft=ft_koja, \\\n",
    "    pre_d=koja_d, ft_d=koja_d, \\\n",
    "    pre_sentences=ja_jabart_sentences[-100:], ft_sentences=ko_jabart_sentences[-100:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "atomic-homeless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.39813758667059446\n",
      "Layer 1 0.8372665742577943\n",
      "Layer 2 0.884557799175998\n",
      "Layer 3 0.9167199884185456\n",
      "Layer 4 0.9193502656476504\n",
      "Layer 5 0.9218732232526243\n",
      "Layer 6 0.9340356606040892\n",
      "Layer 7 0.8588061707660819\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.18737031889213474\n",
      "Layer 1 0.27824082522955756\n",
      "Layer 2 0.2912734439356014\n",
      "Layer 3 0.3114805956735732\n",
      "Layer 4 0.3444135556508579\n",
      "Layer 5 0.293009997229996\n",
      "Layer 6 0.0256687581392078\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim_v2(pre=bart_jako, ft=ft_koja, \\\n",
    "                  pre_d=koja_d, ft_d=koja_d, \\\n",
    "                  pre_sentences=koja_sentences[:100], ft_sentences=jako_sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "based-tourism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.38914966417195\n",
      "Layer 1 0.8772289821922453\n",
      "Layer 2 0.8899276493629373\n",
      "Layer 3 0.9037599286008311\n",
      "Layer 4 0.9058940246759116\n",
      "Layer 5 0.9117364861394909\n",
      "Layer 6 0.9338582542674801\n",
      "Layer 7 0.8843983840721953\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.8255794427451447\n",
      "Layer 1 0.7307286521922709\n",
      "Layer 2 0.7551462865527659\n",
      "Layer 3 0.7159976288355673\n",
      "Layer 4 0.7050116593377683\n",
      "Layer 5 0.7698804841836703\n",
      "Layer 6 0.48371331175894106\n"
     ]
    }
   ],
   "source": [
    "# koja, jako\n",
    "encdec_cka_sim(pre=ft_jako, ft=ft_koja, pre_d=koja_d, ft_d=koja_d, pre_sentences=koja_sentences[:100], ft_sentences=jako_sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "destroyed-window",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9764038300133336\n",
      "Layer 1 0.9262711358440008\n",
      "Layer 2 0.8891990692662358\n",
      "Layer 3 0.9495674060863042\n",
      "Layer 4 0.9690189805392235\n",
      "Layer 5 0.9769565647363784\n",
      "Layer 6 0.9769848289594428\n",
      "Layer 7 0.8691340463003271\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.2885973356828191\n",
      "Layer 1 0.561022844039608\n",
      "Layer 2 0.5292809256793927\n",
      "Layer 3 0.6442883136936155\n",
      "Layer 4 0.7444158038803855\n",
      "Layer 5 0.6832025402794522\n",
      "Layer 6 0.06761801081006032\n"
     ]
    }
   ],
   "source": [
    "# jako, jaBART\n",
    "encdec_cka_sim(pre=bart_jako, ft=ft_jako, pre_d=koja_d, ft_d=koja_d, sentences=koja_sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "premier-somalia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9897062022986469\n",
      "Layer 1 0.942404347304907\n",
      "Layer 2 0.9360558006020253\n",
      "Layer 3 0.9749580252041363\n",
      "Layer 4 0.9854106431689292\n",
      "Layer 5 0.9884419785774842\n",
      "Layer 6 0.9847088999092097\n",
      "Layer 7 0.8975083491452324\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.7752163404416937\n",
      "Layer 1 0.7797717512225465\n",
      "Layer 2 0.8455108030791875\n",
      "Layer 3 0.8263716658756463\n",
      "Layer 4 0.7835157133452053\n",
      "Layer 5 0.7382126413634155\n",
      "Layer 6 0.41533271093317037\n"
     ]
    }
   ],
   "source": [
    "# jako, jaen\n",
    "encdec_cka_sim(pre=ft_jako, ft=ft_jaen, pre_d=koja_d, ft_d=enja_d, sentences=koja_sentences[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-monster",
   "metadata": {},
   "source": [
    "## enBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "large-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "Encoder CKA\n",
      "Layer 0 0.9710718630659055\n",
      "Layer 1 0.8120210146657719\n",
      "Layer 2 0.8006389337341203\n",
      "Layer 3 0.7654746487379535\n",
      "Layer 4 0.7380350953301268\n",
      "Layer 5 0.7333817872153834\n",
      "Layer 6 0.7386213187787545\n",
      "Layer 7 0.7386213187787545\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.06769563267976879\n",
      "Layer 1 0.12355517600508875\n",
      "Layer 2 0.1915150468153713\n",
      "Layer 3 0.23190734801412766\n",
      "Layer 4 0.22419532676016332\n",
      "Layer 5 0.24174943072293711\n",
      "Layer 6 0.4389424708551309\n"
     ]
    }
   ],
   "source": [
    "# enBART, enja\n",
    "encdec_cka_sim(\n",
    "    pre=enbart_enja, ft=enbart_ft_enja, \\\n",
    "    pre_d=enja_enbart_d, ft_d=enja_enbart_d, \\\n",
    "    pre_sentences=en_enbart_sentences[:1000], ft_sentences=en_enbart_sentences[:1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "false-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9800959491456113\n",
      "Layer 1 0.9063465790962039\n",
      "Layer 2 0.8859278730303163\n",
      "Layer 3 0.8680285143785702\n",
      "Layer 4 0.8561370800893219\n",
      "Layer 5 0.8507612922158924\n",
      "Layer 6 0.8457999253847499\n",
      "Layer 7 0.8457999253847499\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.18947321394560013\n",
      "Layer 1 0.29860176561342905\n",
      "Layer 2 0.4003624704114809\n",
      "Layer 3 0.45474652714640984\n",
      "Layer 4 0.41357241889775465\n",
      "Layer 5 0.38817485409671454\n",
      "Layer 6 0.6640995950456667\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim(pre=ft_enja_enbart, ft=bart_ende, pre_d=enja_enbart_d, ft_d=ende_d, sentences=en_enbart_sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "certified-transsexual",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 200\n",
      "done 400\n",
      "done 600\n",
      "done 800\n",
      "done 1000\n",
      "Encoder CKA\n",
      "Layer 0 0.3266183627050019\n",
      "Layer 1 0.5554550327393063\n",
      "Layer 2 0.5996869103216395\n",
      "Layer 3 0.6228156618648397\n",
      "Layer 4 0.6345839533936144\n",
      "Layer 5 0.5826970221310772\n",
      "Layer 6 0.6178093575017386\n",
      "Layer 7 0.6178093575017386\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.10103534028961636\n",
      "Layer 1 0.15979227332956158\n",
      "Layer 2 0.21093274378601296\n",
      "Layer 3 0.2341980910356678\n",
      "Layer 4 0.2375313805561715\n",
      "Layer 5 0.26754144559807225\n",
      "Layer 6 0.4110353001871192\n"
     ]
    }
   ],
   "source": [
    "# enBART, jaen\n",
    "encdec_cka_sim(\n",
    "    pre=enbart_enja, ft=enbart_ft_jaen, \\\n",
    "    pre_d=enja_enbart_d, ft_d=enja_enbart_d, \\\n",
    "    pre_sentences=en_enbart_sentences[:1000], ft_sentences=ja_enbart_sentences[:1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "peaceful-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.4826819965841025\n",
      "Layer 1 0.7448068701601226\n",
      "Layer 2 0.7608820498717873\n",
      "Layer 3 0.774781274391684\n",
      "Layer 4 0.7782604994262258\n",
      "Layer 5 0.7560034084652245\n",
      "Layer 6 0.7619852822661689\n",
      "Layer 7 0.7619852822661689\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.24741065684638075\n",
      "Layer 1 0.358124774393418\n",
      "Layer 2 0.4407861368273513\n",
      "Layer 3 0.486088296263293\n",
      "Layer 4 0.47788928752847304\n",
      "Layer 5 0.47284600780030955\n",
      "Layer 6 0.6871021252518612\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim(pre=ft_jaen_enbart, ft=bart_ende, \\\n",
    "               pre_d=enja_enbart_d, ft_d=ende_d, \\\n",
    "               pre_sentences=ja_enbart_sentences[:100], ft_sentences=en_enbart_sentences[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "optimum-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder CKA\n",
      "Layer 0 0.9840656777237077\n",
      "Layer 1 0.5583370897039305\n",
      "Layer 2 0.505575642587246\n",
      "Layer 3 0.41657622538682904\n",
      "Layer 4 0.3298949865619421\n",
      "Layer 5 0.2737503780868102\n",
      "Layer 6 0.23969355799771241\n",
      "Layer 7 0.23969355799771241\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.09492613189329874\n",
      "Layer 1 0.13209263859133985\n",
      "Layer 2 0.16080890859662506\n",
      "Layer 3 0.17255819495854252\n",
      "Layer 4 0.174431708171571\n",
      "Layer 5 0.20196991993492058\n",
      "Layer 6 0.29839598438325493\n"
     ]
    }
   ],
   "source": [
    "encdec_cka_sim(pre=ft_jaen_enbart, ft=bart_ende, \\\n",
    "               pre_d=enja_enbart_d, ft_d=ende_d, \\\n",
    "               pre_sentences=en_enbart_sentences[:100], ft_sentences=en_enbart_sentences[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-photography",
   "metadata": {},
   "source": [
    "## ENJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "orange-series",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models\n"
     ]
    }
   ],
   "source": [
    "bart_enja_path = \"../../jpBART/en-ja/ja-bart\"\n",
    "bart_enja_name = \"model.pt\"\n",
    "bart_enja = load_bart(path=bart_enja_path, model_name=bart_enja_name)\n",
    "bart_enja.to(DEVICE)\n",
    "\n",
    "ft_jaen_path = \"../../jpBART/en-ja/jaen/ft_model\"\n",
    "ft_jaen_name = \"checkpoint_best.pt\"\n",
    "ft_jaen = load_bart(path=ft_jaen_path, model_name=ft_jaen_name)\n",
    "ft_jaen.to(DEVICE)\n",
    "\n",
    "print(\"loaded models\")\n",
    "\n",
    "enja_d = load_dict(\"../../jpBART/en-ja/ja-bart/dict.txt\")\n",
    "file_path = \"../../jpBART/en-ja/data/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    enja_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "premier-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../jpBART/en-ja/data/dev.en\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    jaen_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impossible-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models\n"
     ]
    }
   ],
   "source": [
    "ft_enja_path = \"../../jpBART/en-ja/ft_model\"\n",
    "ft_enja_name = \"checkpoint_best.pt\"\n",
    "ft_enja = load_bart(path=ft_enja_path, model_name=ft_enja_name)\n",
    "ft_enja.to(DEVICE)\n",
    "print(\"loaded models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gothic-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../jpBART/data/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    koja_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "golden-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../jpBART/data/dev.ko\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    jako_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-intranet",
   "metadata": {},
   "source": [
    "## KOJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "reverse-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models\n"
     ]
    }
   ],
   "source": [
    "bart_jako_path = \"../../jpBART/japanese_bart_base_1.1/test\"\n",
    "bart_jako_name = \"model.pt\"\n",
    "ft_jako_path = \"../../jpBART/ja-ko/bart/ft/\"\n",
    "ft_jako_name = \"checkpoint_best.pt\"\n",
    "bart_jako = load_bart(path=bart_jako_path, model_name=bart_jako_name)\n",
    "bart_jako.to(DEVICE)\n",
    "ft_jako = load_bart(path=ft_jako_path, model_name=ft_jako_name)\n",
    "ft_jako.to(DEVICE)\n",
    "koja_d = load_dict(\"../../jpBART/japanese_bart_base_1.1/test/dict.txt\")\n",
    "print(\"loaded models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "golden-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models\n"
     ]
    }
   ],
   "source": [
    "ft_koja_path = \"../../jpBART/v3/ft\"\n",
    "ft_koja_name = \"checkpoint_best.pt\"\n",
    "ft_koja = load_bart(path=ft_koja_path, model_name=ft_koja_name)\n",
    "ft_koja.to(DEVICE)\n",
    "print(\"loaded models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-white",
   "metadata": {},
   "source": [
    "## JAJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "southeast-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models\n"
     ]
    }
   ],
   "source": [
    "ft_jaja_path = \"../../jpBART/jaja/bart/ft\"\n",
    "ft_jaja_name = \"checkpoint_best.pt\"\n",
    "ft_jaja = load_bart(path=ft_jaja_path, model_name=ft_jaja_name)\n",
    "ft_jaja.to(DEVICE)\n",
    "print(\"loaded models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
