{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excessive-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 5.0 MB/s eta 0:00:01    |██████████▋                     | 3.7 MB 5.0 MB/s eta 0:00:02     |█████████████████████████████▉  | 10.5 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.3\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "technological-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import torch \n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sys.path.append('../tools')\n",
    "from generate import generate, sent_to_ids, load_bart, load_sents, load_dict, ids_to_tokens\n",
    "from cka import encdec_cka_sim, cal_encdec_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patient-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(sents1, sents2, num):\n",
    "    sents = [(sent1, sent2) for sent1, sent2 in zip(sents1, sents2)]\n",
    "    sampled_sents = random.sample(sents, num)\n",
    "    sampled_sents1 = [x[0] for x in sampled_sents]\n",
    "    sampled_sents2 = [x[1] for x in sampled_sents]\n",
    "    return sampled_sents1, sampled_sents2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-sheet",
   "metadata": {},
   "source": [
    "# Japanese BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stopped-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained JaBART\n",
    "jabart_jako_path = \"../pretrained_bart/trim/jabart_jako\"\n",
    "jabart_jako_name = \"ja_bart_base.pt\"\n",
    "pre_model = load_bart(\n",
    "    path=jabart_jako_path, model_name=jabart_jako_name\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-cache",
   "metadata": {},
   "source": [
    "## Korean/Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "relative-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "koja_d = load_dict(\"../pretrained_bart/trim/jabart_jako/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/dev.ko\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_ko = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indian-motor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9949951773347993\n",
      "Layer 1 0.978321613472265\n",
      "Layer 2 0.9707902466643927\n",
      "Layer 3 0.9646707396818045\n",
      "Layer 4 0.9646002217313576\n",
      "Layer 5 0.9685885867051048\n",
      "Layer 6 0.9706604459363375\n",
      "Layer 7 0.9782752876191654\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.12777481907275934\n",
      "Layer 1 0.29481993677082186\n",
      "Layer 2 0.35793248291284\n",
      "Layer 3 0.3891760720934722\n",
      "Layer 4 0.4576973959278487\n",
      "Layer 5 0.44532457896215877\n",
      "Layer 6 0.15896837357511515\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.19339110636848317\n",
      "Layer 1 0.33576518366058417\n",
      "Layer 2 0.41018147017254963\n",
      "Layer 3 0.42937301169240033\n",
      "Layer 4 0.44847606941201223\n",
      "Layer 5 0.393520348780849\n",
      "elapsed_time:111.73702645301819[sec]\n",
      "\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9971703786815697\n",
      "Layer 1 0.9812738013289333\n",
      "Layer 2 0.9739238246902284\n",
      "Layer 3 0.9650739860483872\n",
      "Layer 4 0.9618400017708343\n",
      "Layer 5 0.9660422979403386\n",
      "Layer 6 0.968122705004213\n",
      "Layer 7 0.9782481944354261\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.12138005263053207\n",
      "Layer 1 0.2570982162304512\n",
      "Layer 2 0.317451346899074\n",
      "Layer 3 0.34108509333253895\n",
      "Layer 4 0.38336732931309214\n",
      "Layer 5 0.3730877445713164\n",
      "Layer 6 0.14565507281983664\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.1858750434836678\n",
      "Layer 1 0.30689625563559614\n",
      "Layer 2 0.3483071697250494\n",
      "Layer 3 0.375189978250909\n",
      "Layer 4 0.3877989131905401\n",
      "Layer 5 0.3236910988351791\n",
      "elapsed_time:113.68849682807922[sec]\n"
     ]
    }
   ],
   "source": [
    "# ja-ko\n",
    "ft_path = \"../ja-ko/bart/checkpoints_\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=koja_d, \n",
    "    pre_sents=sentences_ja[:100], ft_sents=sentences_ja[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "print()\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_ja = random_sampling(\n",
    "    sentences_ja, sentences_ja, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=koja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_ja,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fluid-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.8670065408395228\n",
      "Layer 1 0.8895461912213435\n",
      "Layer 2 0.8873928891970647\n",
      "Layer 3 0.8871296692502944\n",
      "Layer 4 0.8912271119372739\n",
      "Layer 5 0.8894015121453398\n",
      "Layer 6 0.8686754130533374\n",
      "Layer 7 0.8919541208527568\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.14147046357928023\n",
      "Layer 1 0.26892974630178157\n",
      "Layer 2 0.33252863252456805\n",
      "Layer 3 0.365625348645496\n",
      "Layer 4 0.4467685761657224\n",
      "Layer 5 0.42780138813384616\n",
      "Layer 6 0.17588759510507052\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.20081955237683285\n",
      "Layer 1 0.3132207114217684\n",
      "Layer 2 0.3832599460245699\n",
      "Layer 3 0.4211763134670955\n",
      "Layer 4 0.4506675530866451\n",
      "Layer 5 0.37938939485516543\n",
      "elapsed_time:108.37133860588074[sec]\n",
      "\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.8828052578510951\n",
      "Layer 1 0.9040147464892014\n",
      "Layer 2 0.8906150178104792\n",
      "Layer 3 0.8838428647354586\n",
      "Layer 4 0.8944273267348168\n",
      "Layer 5 0.9063596033533926\n",
      "Layer 6 0.8884562648335584\n",
      "Layer 7 0.8915533906290435\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.15945214045884482\n",
      "Layer 1 0.2301107814456214\n",
      "Layer 2 0.2810259877645324\n",
      "Layer 3 0.30112129693878437\n",
      "Layer 4 0.3413878644049451\n",
      "Layer 5 0.3025392750966902\n",
      "Layer 6 0.12649043719278555\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.20187657079827914\n",
      "Layer 1 0.2768636300866152\n",
      "Layer 2 0.3128932577278696\n",
      "Layer 3 0.33865115253137784\n",
      "Layer 4 0.3419903773080327\n",
      "Layer 5 0.2811961032945658\n",
      "elapsed_time:110.05219507217407[sec]\n"
     ]
    }
   ],
   "source": [
    "# ko-ja\n",
    "ft_path = \"../ko-ja/bart/checkpoints_\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=koja_d, \n",
    "    pre_sents=sentences_ja[:100], ft_sents=sentences_ko[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "print()\n",
    "\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_ko = random_sampling(\n",
    "    sentences_ja, sentences_ko, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=koja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_ko,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-heather",
   "metadata": {},
   "source": [
    "## English/Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "original-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "enja_d = load_dict(\"../pretrained_bart/trim/jabart_jaen/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/enja/dev.ja\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/enja/dev.en\"\n",
    "with open(file_path , \"r\") as f:\n",
    "    sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "usual-amount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9855443313644157\n",
      "Layer 1 0.9842778298120605\n",
      "Layer 2 0.9689511473773461\n",
      "Layer 3 0.9488710523249988\n",
      "Layer 4 0.9355105208633925\n",
      "Layer 5 0.9414986054875325\n",
      "Layer 6 0.9442956888864806\n",
      "Layer 7 0.9594235106182454\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.22672378119550454\n",
      "Layer 1 0.3058987004779762\n",
      "Layer 2 0.3696682207217582\n",
      "Layer 3 0.40461809484243816\n",
      "Layer 4 0.47326659327561416\n",
      "Layer 5 0.35687222645448163\n",
      "Layer 6 0.0452675229508782\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.2742708872149076\n",
      "Layer 1 0.3323725391104788\n",
      "Layer 2 0.39381109783899926\n",
      "Layer 3 0.4624094692957962\n",
      "Layer 4 0.42897521509000486\n",
      "Layer 5 0.2590982724671253\n",
      "elapsed_time:93.67176389694214[sec]\n",
      "\n",
      "Use random sampled sentences\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.989563281497098\n",
      "Layer 1 0.974816472648417\n",
      "Layer 2 0.9528309253401753\n",
      "Layer 3 0.9221236680843773\n",
      "Layer 4 0.905613568223484\n",
      "Layer 5 0.9197068856932752\n",
      "Layer 6 0.9353293294050234\n",
      "Layer 7 0.9439371613914803\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.1119577261480585\n",
      "Layer 1 0.19932022451944195\n",
      "Layer 2 0.29359013945335416\n",
      "Layer 3 0.35911365734848844\n",
      "Layer 4 0.43916214260604197\n",
      "Layer 5 0.3338383694447813\n",
      "Layer 6 0.1503958812159764\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.18135612039168253\n",
      "Layer 1 0.26100318634236164\n",
      "Layer 2 0.34228249247427395\n",
      "Layer 3 0.43092717671459235\n",
      "Layer 4 0.41025013328783927\n",
      "Layer 5 0.30782225709240135\n",
      "elapsed_time:94.3192343711853[sec]\n"
     ]
    }
   ],
   "source": [
    "# ja-en\n",
    "ft_path = \"../ja-en/bart/checkpoints_\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sents=sentences_ja[:100], ft_sents=sentences_ja[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "\n",
    "print('\\nUse random sampled sentences')\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_ja = random_sampling(\n",
    "    sentences_ja, sentences_ja, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_ja,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "typical-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.755642804883526\n",
      "Layer 1 0.7259107216948855\n",
      "Layer 2 0.7298314655374136\n",
      "Layer 3 0.7247183757701026\n",
      "Layer 4 0.7424009289116854\n",
      "Layer 5 0.7604993429803047\n",
      "Layer 6 0.748688414409073\n",
      "Layer 7 0.7792510965659427\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.25251256084564844\n",
      "Layer 1 0.283276554135944\n",
      "Layer 2 0.3501964215181492\n",
      "Layer 3 0.3955378594849012\n",
      "Layer 4 0.43331348061618863\n",
      "Layer 5 0.3106614850973206\n",
      "Layer 6 0.05336002335811889\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.2955529599390394\n",
      "Layer 1 0.3063162122481898\n",
      "Layer 2 0.38072998388793894\n",
      "Layer 3 0.4341193086095668\n",
      "Layer 4 0.39252758036673513\n",
      "Layer 5 0.21263450161342567\n",
      "elapsed_time:87.59473395347595[sec]\n",
      "\n",
      "Use random sampled sentences\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.43952244795753864\n",
      "Layer 1 0.45148635766727835\n",
      "Layer 2 0.45407653326032255\n",
      "Layer 3 0.456910254492851\n",
      "Layer 4 0.44128720776525304\n",
      "Layer 5 0.4321100468523806\n",
      "Layer 6 0.4247472443873235\n",
      "Layer 7 0.49199936471273664\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.2187766210541609\n",
      "Layer 1 0.24496475980236715\n",
      "Layer 2 0.33469679147778675\n",
      "Layer 3 0.368891923542102\n",
      "Layer 4 0.4009444935595677\n",
      "Layer 5 0.31946105639655187\n",
      "Layer 6 0.059528893832202785\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.27436065973329726\n",
      "Layer 1 0.31041440068043336\n",
      "Layer 2 0.3700385357524329\n",
      "Layer 3 0.3913675411483885\n",
      "Layer 4 0.3791862244039186\n",
      "Layer 5 0.2610391434145364\n",
      "elapsed_time:90.11960744857788[sec]\n"
     ]
    }
   ],
   "source": [
    "# en-ja\n",
    "ft_path = \"../en-ja/bart/checkpoints_\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sents=sentences_ja[:100], ft_sents=sentences_en[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "\n",
    "print('\\nUse random sampled sentences')\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_en = random_sampling(\n",
    "    sentences_ja, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=koja_d, ft_d=enja_d, \n",
    "    pre_sents=sampled_sents_ja, ft_sents=sampled_sents_en,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-sight",
   "metadata": {},
   "source": [
    "# English BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ignored-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained EnBART\n",
    "enbart_enja_path = \"../pretrained_bart/trim/enbart_jaen\"\n",
    "enbart_enja_name = \"jaen_en_bart_base.pt\"\n",
    "pre_model = load_bart(\n",
    "    path=enbart_enja_path, model_name=enbart_enja_name\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-clearing",
   "metadata": {},
   "source": [
    "## Japanese/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hydraulic-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "enja_d = load_dict(\"../pretrained_bart/trim/enbart_jaen/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/enja_2/enBART/dev.ja\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_ja = f.readlines()\n",
    "\n",
    "file_path = \"../data/enja_2/enBART/dev.en\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "closed-blank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.654298177716511\n",
      "Layer 1 0.7688231598694694\n",
      "Layer 2 0.7613042981440016\n",
      "Layer 3 0.7550312213702418\n",
      "Layer 4 0.759050423254224\n",
      "Layer 5 0.7555643831752289\n",
      "Layer 6 0.7405338358666992\n",
      "Layer 7 0.7405338358666992\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.11193837400885838\n",
      "Layer 1 0.1367184881352814\n",
      "Layer 2 0.1873775117338924\n",
      "Layer 3 0.2853384773446727\n",
      "Layer 4 0.20546009958102718\n",
      "Layer 5 0.13299156414864938\n",
      "Layer 6 0.449261108132735\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.11247151019520017\n",
      "Layer 1 0.21111405889740117\n",
      "Layer 2 0.3054620242484335\n",
      "Layer 3 0.3228863073744893\n",
      "Layer 4 0.2926185589791821\n",
      "Layer 5 0.1741042261593089\n",
      "elapsed_time:89.07417297363281[sec]\n",
      "\n",
      "Use random sampled sentences\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.44308230789884906\n",
      "Layer 1 0.5502702209143098\n",
      "Layer 2 0.5519916281037587\n",
      "Layer 3 0.5563345593379978\n",
      "Layer 4 0.5585082009441193\n",
      "Layer 5 0.5547125939640384\n",
      "Layer 6 0.5425837385997123\n",
      "Layer 7 0.5425837385997123\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.09931835963931872\n",
      "Layer 1 0.07595272749699429\n",
      "Layer 2 0.11041319036774082\n",
      "Layer 3 0.1846376892397597\n",
      "Layer 4 0.13568210247405774\n",
      "Layer 5 0.08839916309783269\n",
      "Layer 6 0.3417472827864746\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.08764377269044171\n",
      "Layer 1 0.15421076094913194\n",
      "Layer 2 0.22038680891153356\n",
      "Layer 3 0.23608631584197678\n",
      "Layer 4 0.20428137809692784\n",
      "Layer 5 0.1089702591060719\n",
      "elapsed_time:87.87845849990845[sec]\n"
     ]
    }
   ],
   "source": [
    "# ja-en\n",
    "ft_path = \"../ja-en/enbart/v2/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enja_d, \n",
    "    pre_sents=sentences_en[:100], ft_sents=sentences_ja[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "\n",
    "print('\\nUse random sampled sentences')\n",
    "random.seed(1)\n",
    "sampled_sents_ja, sampled_sents_en = random_sampling(\n",
    "    sentences_ja, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enja_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_ja,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "approved-stevens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9965538477834348\n",
      "Layer 1 0.975280185956344\n",
      "Layer 2 0.9437826740859443\n",
      "Layer 3 0.9362823855683542\n",
      "Layer 4 0.9306819536456685\n",
      "Layer 5 0.9308850844551042\n",
      "Layer 6 0.9296681597115906\n",
      "Layer 7 0.9296681597115906\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.11343461072372857\n",
      "Layer 1 0.15391131681694342\n",
      "Layer 2 0.1982631370654231\n",
      "Layer 3 0.23597873938762637\n",
      "Layer 4 0.279553364208512\n",
      "Layer 5 0.30365452656952513\n",
      "Layer 6 0.6037295704306445\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.12466590918386924\n",
      "Layer 1 0.16705264895038846\n",
      "Layer 2 0.19181686344986004\n",
      "Layer 3 0.2722902299273229\n",
      "Layer 4 0.2996745973027566\n",
      "Layer 5 0.3568976167906651\n",
      "elapsed_time:143.360004901886[sec]\n",
      "\n",
      "Use random sampled sentences\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9947663859591692\n",
      "Layer 1 0.9690050812960682\n",
      "Layer 2 0.9267659871455628\n",
      "Layer 3 0.9259282590568918\n",
      "Layer 4 0.9225745072622071\n",
      "Layer 5 0.9233199679951328\n",
      "Layer 6 0.9257420342116391\n",
      "Layer 7 0.9257420342116391\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.06200720237022637\n",
      "Layer 1 0.09602597670796026\n",
      "Layer 2 0.1162435341810525\n",
      "Layer 3 0.1489488699434853\n",
      "Layer 4 0.1765507707264187\n",
      "Layer 5 0.20782397281105974\n",
      "Layer 6 0.5513750252869679\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.07340509526139198\n",
      "Layer 1 0.10115642577108794\n",
      "Layer 2 0.12500983513936922\n",
      "Layer 3 0.1745240158137556\n",
      "Layer 4 0.22196037311379893\n",
      "Layer 5 0.260123068278414\n",
      "elapsed_time:144.30706882476807[sec]\n"
     ]
    }
   ],
   "source": [
    "# en-ja\n",
    "ft_path = \"../en-ja/enbart/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enja_d, \n",
    "    pre_sents=sentences_en[:100], ft_sents=sentences_en[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "\n",
    "print('\\nUse random sampled sentences')\n",
    "random.seed(1)\n",
    "sampled_sents_en, sampled_sents_en = random_sampling(\n",
    "    sentences_en, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enja_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_en,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-sympathy",
   "metadata": {},
   "source": [
    "## French/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "favorite-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict\n",
    "enfr_d = load_dict(\"../pretrained_bart/trim/enbart_fren/dict.txt\")\n",
    "\n",
    "# sentences \n",
    "file_path = \"../data/enfr/random/dev.fr\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_fr = f.readlines()\n",
    "\n",
    "file_path = \"../data/enfr/random/dev.en\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    sentences_en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "front-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.7471251485767328\n",
      "Layer 1 0.7778515655411314\n",
      "Layer 2 0.7695343506641981\n",
      "Layer 3 0.7402289693809095\n",
      "Layer 4 0.7444042194525834\n",
      "Layer 5 0.7603124588891431\n",
      "Layer 6 0.7451640155456389\n",
      "Layer 7 0.7451640155456389\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.0865684785174013\n",
      "Layer 1 0.08488433385486086\n",
      "Layer 2 0.1574676924823072\n",
      "Layer 3 0.23263037164746725\n",
      "Layer 4 0.277752855277543\n",
      "Layer 5 0.2747906411352265\n",
      "Layer 6 0.38647164713488663\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.08890460576881483\n",
      "Layer 1 0.15645316283939398\n",
      "Layer 2 0.24675014853677568\n",
      "Layer 3 0.31262805062130006\n",
      "Layer 4 0.3556898458948217\n",
      "Layer 5 0.28861438428914443\n",
      "elapsed_time:93.26017045974731[sec]\n",
      "\n",
      "Use random sampled sentences\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.7574330674767051\n",
      "Layer 1 0.801269538625719\n",
      "Layer 2 0.7998796712120713\n",
      "Layer 3 0.7890301957637343\n",
      "Layer 4 0.7976716919757547\n",
      "Layer 5 0.8015287774615393\n",
      "Layer 6 0.7860287601214588\n",
      "Layer 7 0.7860287601214588\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.11404063535437517\n",
      "Layer 1 0.13366531352690927\n",
      "Layer 2 0.20974278500571297\n",
      "Layer 3 0.28380402251181047\n",
      "Layer 4 0.34547137075906836\n",
      "Layer 5 0.33560382052917476\n",
      "Layer 6 0.33809475446361426\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.1125478520801042\n",
      "Layer 1 0.20930441798997027\n",
      "Layer 2 0.3089278955797583\n",
      "Layer 3 0.3686612789732633\n",
      "Layer 4 0.38352969131256676\n",
      "Layer 5 0.3145672434824055\n",
      "elapsed_time:95.90175914764404[sec]\n"
     ]
    }
   ],
   "source": [
    "# fr-en\n",
    "ft_path = \"../fr-en/bart/1M/v2/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enfr_d, \n",
    "    pre_sents=sentences_en[:100], ft_sents=sentences_fr[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "\n",
    "\n",
    "print('\\nUse random sampled sentences')\n",
    "random.seed(1)\n",
    "sampled_sents_en, sampled_sents_fr = random_sampling(\n",
    "    sentences_en, sentences_fr, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enfr_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_fr,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecological-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9860679393196332\n",
      "Layer 1 0.8838703663785191\n",
      "Layer 2 0.8528427986416215\n",
      "Layer 3 0.8147620972026153\n",
      "Layer 4 0.7997088820301695\n",
      "Layer 5 0.798447798704826\n",
      "Layer 6 0.7600422429275261\n",
      "Layer 7 0.7600422429275261\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.08753203404535702\n",
      "Layer 1 0.0921635522476793\n",
      "Layer 2 0.1522007049912804\n",
      "Layer 3 0.20841556070939177\n",
      "Layer 4 0.24060344570743167\n",
      "Layer 5 0.2382468474216198\n",
      "Layer 6 0.2892679693610796\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.09026119869653651\n",
      "Layer 1 0.15120692696250757\n",
      "Layer 2 0.2189137682652893\n",
      "Layer 3 0.26889816993114063\n",
      "Layer 4 0.2831166777517662\n",
      "Layer 5 0.2377757804361311\n",
      "elapsed_time:101.05390071868896[sec]\n",
      "\n",
      "Use random sampled sentences\n",
      "(8, 100, 768) (7, 100, 768) (6, 100, 768)\n",
      "(8, 100, 768) (7, 100, 768)\n",
      "Encoder CKA\n",
      "Layer 0 0.9771762965273146\n",
      "Layer 1 0.9073712394139727\n",
      "Layer 2 0.8965725297233069\n",
      "Layer 3 0.8815354150381705\n",
      "Layer 4 0.8775547720352443\n",
      "Layer 5 0.872442251598707\n",
      "Layer 6 0.8410559203035358\n",
      "Layer 7 0.8410559203035358\n",
      "\n",
      "Decoder CKA\n",
      "Layer 0 0.10778351003020133\n",
      "Layer 1 0.12908465704311187\n",
      "Layer 2 0.19107924906277546\n",
      "Layer 3 0.2578092991278896\n",
      "Layer 4 0.29301651690077135\n",
      "Layer 5 0.28792259101580175\n",
      "Layer 6 0.26776594653292807\n",
      "\n",
      "Decoder up to self attention CKA\n",
      "Layer 0 0.10354058602131933\n",
      "Layer 1 0.18931631487983047\n",
      "Layer 2 0.2836285855859728\n",
      "Layer 3 0.3269118008003395\n",
      "Layer 4 0.3352472237992844\n",
      "Layer 5 0.2687337239909468\n",
      "elapsed_time:105.76447820663452[sec]\n"
     ]
    }
   ],
   "source": [
    "# en-fr\n",
    "ft_path = \"../en-fr/bart/1M/checkpoints\"\n",
    "ft_name = \"checkpoint_best.pt\"\n",
    "ft_model = load_bart(\n",
    "    path=ft_path, model_name=ft_name\n",
    ").to(DEVICE)\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enfr_d, \n",
    "    pre_sents=sentences_en[:100], ft_sents=sentences_en[:100],\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")\n",
    "\n",
    "print('\\nUse random sampled sentences')\n",
    "random.seed(1)\n",
    "sampled_sents_en, sampled_sents_en = random_sampling(\n",
    "    sentences_en, sentences_en, 100\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "encdec_cka_sim(\n",
    "    pre=pre_model, ft=ft_model, \n",
    "    pre_d=enja_d, ft_d=enfr_d,  \n",
    "    pre_sents=sampled_sents_en, ft_sents=sampled_sents_en,\n",
    "    batch_size=16\n",
    ")\n",
    "end = time.time()\n",
    "print (f\"elapsed_time:{end-start}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-minimum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
